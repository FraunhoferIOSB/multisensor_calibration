{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Multi-Sensor Calibration Toolbox \u00b6 With a graphical user interface the Multi-Sensor Calibration Toolbox guides the user through the calibration of a camera-LiDAR sensor setup. About \u00b6 multisensor_calibration is an actively maintained universal calibration toolbox for assisted, target-based multi-sensor calibration with ROS 1 and ROS 2 support. It provides a variety of methods and applications to calibrate complex multi-sensor systems, e.g. Extrinsic Camera-LiDAR Calibration , Extrinsic Camera-Reference Calibration , Extrinsic LiDAR-LiDAR Calibration , Extrinsic LiDAR-Reference Calibration , and Extrinsic LiDAR-Vehicle Calibration (prototype). The software is licensed under the new BSD 3-Clause license . If you use this project for your research, please cite: @inproceedings{ ruf2025_multisensor_calibration, title={Multi-Sensor Calibration Toolbox for Large-Scale Offroad Robotics}, author={Boitumelo Ruf and Miguel Granero and Raphael Hagmanns and Janko Petereit}, conference={German Robotics Conference (GRC) 2025}, year={2025}, } The latest source code is available on GitHub . Furthermore, the multisensor_calibration is also available as an official package for ROS 2 and be installable with apt. Since ROS 1 is soon end-of-life, there will be no official release for ROS 1. However, there is a version of the source code available for ROS 1 under the branch noetic . Getting Started \u00b6 See our Installation page on how to install it with apt-get as ROS 2 package or how to build from source. Look at the Tutorial page on how to use use it to calibrate your sensor setup. Support \u00b6 Please, use GitHub Discussions for questions and the GitHub issue tracker for bug reports, feature requests/additions, etc. Or look at the troubleshooting page. To get information on how to contribute, please look at our contribution guide . Acknowledgements \u00b6 This software was developed as part of the projects AKIT-PRO (grant no. 13N15673) and ROBDEKON \u2013 Robotic Systems for Decontamination in Hazardous Environments (grant nos. 13N14674 and 13N16538), funded by the Federal Ministry of Education and Research (BMBF) under the German Federal Government\u2019s Research for Civil Security program. The Multi-Sensor Calibration Toolbox was originally developed by Boitumelo Ruf . It is currently mainly maintained by Miguel Granero supported by other members of the Autonomous Robotic Systems group from the Fraunhofer IOSB","title":"Multi-Sensor Calibration Toolbox"},{"location":"#multi-sensor-calibration-toolbox","text":"With a graphical user interface the Multi-Sensor Calibration Toolbox guides the user through the calibration of a camera-LiDAR sensor setup.","title":"Multi-Sensor Calibration Toolbox"},{"location":"#about","text":"multisensor_calibration is an actively maintained universal calibration toolbox for assisted, target-based multi-sensor calibration with ROS 1 and ROS 2 support. It provides a variety of methods and applications to calibrate complex multi-sensor systems, e.g. Extrinsic Camera-LiDAR Calibration , Extrinsic Camera-Reference Calibration , Extrinsic LiDAR-LiDAR Calibration , Extrinsic LiDAR-Reference Calibration , and Extrinsic LiDAR-Vehicle Calibration (prototype). The software is licensed under the new BSD 3-Clause license . If you use this project for your research, please cite: @inproceedings{ ruf2025_multisensor_calibration, title={Multi-Sensor Calibration Toolbox for Large-Scale Offroad Robotics}, author={Boitumelo Ruf and Miguel Granero and Raphael Hagmanns and Janko Petereit}, conference={German Robotics Conference (GRC) 2025}, year={2025}, } The latest source code is available on GitHub . Furthermore, the multisensor_calibration is also available as an official package for ROS 2 and be installable with apt. Since ROS 1 is soon end-of-life, there will be no official release for ROS 1. However, there is a version of the source code available for ROS 1 under the branch noetic .","title":"About"},{"location":"#getting-started","text":"See our Installation page on how to install it with apt-get as ROS 2 package or how to build from source. Look at the Tutorial page on how to use use it to calibrate your sensor setup.","title":"Getting Started"},{"location":"#support","text":"Please, use GitHub Discussions for questions and the GitHub issue tracker for bug reports, feature requests/additions, etc. Or look at the troubleshooting page. To get information on how to contribute, please look at our contribution guide .","title":"Support"},{"location":"#acknowledgements","text":"This software was developed as part of the projects AKIT-PRO (grant no. 13N15673) and ROBDEKON \u2013 Robotic Systems for Decontamination in Hazardous Environments (grant nos. 13N14674 and 13N16538), funded by the Federal Ministry of Education and Research (BMBF) under the German Federal Government\u2019s Research for Civil Security program. The Multi-Sensor Calibration Toolbox was originally developed by Boitumelo Ruf . It is currently mainly maintained by Miguel Granero supported by other members of the Autonomous Robotic Systems group from the Fraunhofer IOSB","title":"Acknowledgements"},{"location":"SUMMARY/","text":"Installation Tutorial Calibration Target Data Processing Workspace Handling Nodes and Nodelets Troubleshooting License Changelog Contributing","title":"SUMMARY"},{"location":"calibration_target/","text":"Calibration Target \u00b6 For extrinsic Multi-Sensor Calibration the toolbox was developed to use an asymmetric calibration target as depicted below. The asymmetry allows to detect a full 6-DOF pose of the target within the sensor data, reducing possible ambiguities in the correspondence search and making the calibration process more robust. Figure 1: Picture of the asymmetric calibration target used for extrinsic calibration. The default calibration target used by 'multisensor_calibration' has the following properties: Width: 1.2m Height: 0.8m Three Circular Cutouts with a Radius: 0.12m located plus-minus 0.15m in X-Y direction around the center. Four ArUco Markers from DICT_6X6_250 with the IDs: 1, 2, 3, 4 (arranged clockwise, starting from top-left) with an Edge Length: 0.18m located plus-minus 0.05m in X-Y direction from the respective corner of the board Using a Custom Calibration Target \u00b6 The Multi-Sensor Calibration the toolbox, however, allows to adjust the parameters of the calibration target by editing them within a yaml file: <repository>/mutlisensor_calibration/cfg/TargetWithCirclesAndAruco.yaml %YAML:1.0 board_width: 1.2 # width of board in meters board_height: 0.8 # height of board in meters marker_size: 0.18 # side length of aruco markers in meters marker_ids: # ids of marker used as a single column matrix rows: 4 cols: 1 dt: i data: [1, 2, 3, 4] # marker order goes clockwise, starting from top left of board marker_positions: # x,y marker positions (top-left) on board relative to center, stored row-by-row in meters. x: rightwards, y: upwards rows: 4 cols: 2 dt: f data: [-0.55, 0.35, 0.37, 0.35, 0.37, -0.17, -0.55, -0.17] cutouts: # Cutouts (id + parameters) stored as a single row matrix. x: rightwards, y: upwards rows: 1 cols: 12 dt: f data: [1, -0.15, 0.15, 0.12, # Circular cutout: (id: 1, parameters: {X,Y,Radius}) 1, 0.15, -0.15, 0.12, 1, -0.15, -0.15, 0.12] min_marker_detection: 2 # minimum number of markers that need to be detected in the camera image cad_model_mesh: \"calibration_target_3holes_cad_mesh.ply\" # relative file path to CAD model of the calibration target as mesh cad_model_cloud: \"calibration_target_3holes_cad_cloud.ply\" # relative file path to CAD model of the calibration target as cloud The CAD model mesh/cloud is used for the optimization of the detected target pose by aligning the model data to the segmented point cloud by means of GICP.","title":"Calibration Target"},{"location":"calibration_target/#calibration-target","text":"For extrinsic Multi-Sensor Calibration the toolbox was developed to use an asymmetric calibration target as depicted below. The asymmetry allows to detect a full 6-DOF pose of the target within the sensor data, reducing possible ambiguities in the correspondence search and making the calibration process more robust. Figure 1: Picture of the asymmetric calibration target used for extrinsic calibration. The default calibration target used by 'multisensor_calibration' has the following properties: Width: 1.2m Height: 0.8m Three Circular Cutouts with a Radius: 0.12m located plus-minus 0.15m in X-Y direction around the center. Four ArUco Markers from DICT_6X6_250 with the IDs: 1, 2, 3, 4 (arranged clockwise, starting from top-left) with an Edge Length: 0.18m located plus-minus 0.05m in X-Y direction from the respective corner of the board","title":"Calibration Target"},{"location":"calibration_target/#using-a-custom-calibration-target","text":"The Multi-Sensor Calibration the toolbox, however, allows to adjust the parameters of the calibration target by editing them within a yaml file: <repository>/mutlisensor_calibration/cfg/TargetWithCirclesAndAruco.yaml %YAML:1.0 board_width: 1.2 # width of board in meters board_height: 0.8 # height of board in meters marker_size: 0.18 # side length of aruco markers in meters marker_ids: # ids of marker used as a single column matrix rows: 4 cols: 1 dt: i data: [1, 2, 3, 4] # marker order goes clockwise, starting from top left of board marker_positions: # x,y marker positions (top-left) on board relative to center, stored row-by-row in meters. x: rightwards, y: upwards rows: 4 cols: 2 dt: f data: [-0.55, 0.35, 0.37, 0.35, 0.37, -0.17, -0.55, -0.17] cutouts: # Cutouts (id + parameters) stored as a single row matrix. x: rightwards, y: upwards rows: 1 cols: 12 dt: f data: [1, -0.15, 0.15, 0.12, # Circular cutout: (id: 1, parameters: {X,Y,Radius}) 1, 0.15, -0.15, 0.12, 1, -0.15, -0.15, 0.12] min_marker_detection: 2 # minimum number of markers that need to be detected in the camera image cad_model_mesh: \"calibration_target_3holes_cad_mesh.ply\" # relative file path to CAD model of the calibration target as mesh cad_model_cloud: \"calibration_target_3holes_cad_cloud.ply\" # relative file path to CAD model of the calibration target as cloud The CAD model mesh/cloud is used for the optimization of the detected target pose by aligning the model data to the segmented point cloud by means of GICP.","title":"Using a Custom Calibration Target"},{"location":"data_processing/","text":"Data Processing \u00b6 Calibration Target Detection in the Camera Data \u00b6 The calibration target is detected in the image data by means of the ArUco markers. In this, the functionalities provided by OpenCV are utilized to detect markers and compute the pose of the calibration target with respect to the camera. To deduce the pose of the target from the marker detection, the algorithm requires the intrinsic parameters of the camera, which is why a good intrinsic calibration is vital. The image points of the detected markers are stored to be later used as 2D correspondences as part of the PnP algorithm. Furthermore, with the image positions of the detected markers and the a priori knowledge about the calibration board geometry, a 3D point cloud of the calibration target is also created. The detections, as well as the 6-DOF Pose of the target and the reconstructed 3D point cloud are published after the detection was successful. In this, the marker ID to which the detected corner belongs is stored in the intensity value of the point. Calibration Target Detection in the LiDAR Data \u00b6 The detection and pose estimation of the calibration target within the LiDAR cloud data is realized in a number of consecutive steps. In this numerous algorithms from the Point Cloud Library (PCL) are used. First, the input point cloud is segmented into planer regions using a region growing algorithm. This algorithm requires the point cloud to have normal vectors which are computed prior to the region growing. After that each segmented cluster whether its size and aspect ratio fits to the geometry of the calibration target. If so, the cluster is considered as region of interest (which are also published as preview) in which a detailed detection and pose estimation of the target is performed. In this, the algorithm fits a rotating bounding box to the cluster and checks its dimension. When the actual target detection and pose estimation is to be performed, the orientation of the bounding box is first aligned to the actual orientation of the calibration target using the arrangement of the cutouts. In this, all possible orientations are tested by counting the points in the point cloud at the assumed location of the cutouts. The orientation for which the smallest number of points have been accumulated is taken as the orientation of the target. This pose is used as an initialization to a RANSAC algorithm which tries to fit a custom sample consensus (SAC) model of the calibration target to the segmented region of interest. In this it computes the normal vector of the board based on a random set of points from the cluster and varies position and rotation of the target pose with each RANSAC iteration. Finally the pose with the most inliers is chosen as the winner-takes-it-all solution. As inliers all point that lie on the calibration target are chosen. With each point that falls into the circular cutout, the current estimation is penalized by reducing the number of inliers by a certain factor. When RANSAC has found a pose, the coefficients are optimized by fitting a CAD model into the point cloud using GICP. In this only the points forming the convex hull of the cloud segmented from the input cloud are used. Finally, the 3D coordinates of the marker corners are deduced based on the estimated board pose and stored as 3D correspondences. The detections, as well as the 6-DOF Pose of the target and the segmented 3D point cloud are published after the detection was successful. In this, the marker ID to which the detected corner belongs is stored in the intensity value of the point. The parameters for the algorithm outlined above are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: Input Filter: max_range : Maximum range at which to filter the incoming point cloud prior to any processing. Any point with a range (absolute distance from sensor) larger than max_range will be discarded. Turn to 0 to switch off. Normal Estimation: normal_estimation_search_method : Select method to use for neighbor search. (0 = RadiusSearch, 1 = NearestNeighborSearch) normal_estimation_search_radius : Radius in which to search for neighbors. In case of 'RadiusSearch', this is a spatial extend. In case of 'NearestNeighborSearch', this represents the number of nearest neighbors (truncated to int). Region Growing: region_growing_cluster_size_min : Minimum number of points a cluster needs to contain in order to be considered as valid inside the region growing. region_growing_cluster_size_max : Maximum number of points a cluster needs to contain in order to be considered as valid inside the region growing. region_growing_number_neighbors : Number of neighbor points to consider during region growing. region_growing_angle_thresh : Angle in degrees used as the allowable range for the normals deviation. If the deviation between points normals is less than the smoothness threshold then they are suggested to be in the same cluster. region_growing_curvature_thresh : Second criteria for the region growing. If two points have a small normals deviation then the disparity between their curvatures is tested. Size Filter: size_filter_width_min_tolerance : Tolerance (in m) of the minimum board width when filtering the clusters based on their size. size_filter_width_max_tolerance : Tolerance (in m) of the maximum board width when filtering the clusters based on their size. size_filter_height_min_tolerance : Tolerance (in m) of the minimum board height when filtering the clusters based on their size. size_filter_height_max_tolerance : Tolerance (in m) of the maximum board height when filtering the clusters based on their size. RANSAC: ransac_distance_thresh : Distance threshold (in m) from model for points to count as inliers during RANSAC. ransac_rotation_variance : Maximum angle in rotation (in degrees) to be sampled when computing the new coefficients within RANSAC. ransac_translation_variance : Maximum distance in translation (in m) to be sampled when computing the new coefficients within RANSAC. ransac_optimize_coefficients : Option to activate the optimization of the coefficients by means of ICP. target_icp_variant : Select ICP variant to use to optimize coefficients. (0 = ICP, 1 = PlaneICP, 2 = GICP) target_icp_max_correspondence_distance : Maximum distance for ICP to search for point correspondences. Given as ratio with respect to shorter side of calibration target. target_icp_rotation_tolerance : Rotation tolerance for convergence check. Given in degrees. target_icp_translation_tolerance : Translation tolerance for convergence check. Given in unit of the LiDAR point cloud, typically meters. Perspective-n-Point for 2D-3D Pose Estimation \u00b6 In the extrinsic pose estimation using sets of 2D-3D correspondences (e.g. Camera-LiDAR calibration) the Perspective-n-Point (PnP) algorithm from OpenCV is used. The parameters to configure the PnP algorithm are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: limit_single_board_rpj_error : Use max maximum reprojection error to accept during calibration of a single target pose. If false, 'board_max_rpj_error' is ignored. single_board_max_rpj_error : Limit for maximum reprojection error to accept during calibration of a single target pose. All calibrated poses, that exceed this limit are rejected. single_board_min_inliers : Threshold for minimum number of inliers to accept during calibration of a single target pose. All calibrated poses, that do not reach this threshold are rejected. pnp_inlier_rpj_error_limit : Limit for maximum reprojection error for which points are considered as RANSAC inliers during PnP. GICP for 3D-3D Alignment \u00b6 In the alignment of 3D point clouds of two LiDAR sensors (e.g. LiDAR-LiDAR calibration) the Generalized-ICP (GICP) of 'small_gicp' is used. The parameters to configure the GICP algorithm are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: registration_icp_variant : Select ICP variant to use for registration. (0 = ICP, 1 = PlaneICP, 2 = GICP) registration_icp_max_correspondence_distance : Maximum distance for ICP to search for point correspondences. Given as ratio with respect to shorter side of calibration target. registration_icp_rotation_tolerance : Rotation tolerance for convergence check. Given in degrees. registration_icp_translation_tolerance : Translation tolerance for convergence check. Given in unit of the LiDAR point cloud, typically meters. Error Computation and Estimation of Calibration Certainty \u00b6 In the calibration using the PnP algorithm to align two sensors by means of 2D-3D correspondences, the quality of the calibration is measured by the mean reprojection error .html). For a good calibration a typical reprojection error lies between 2-4 Pixels. In the calibration of two 3D sensors in which the sensor data gets aligned by means of GIPC, the quality of the calibration is measured using the root mean squared error (RMSE) . For a good calibration a typical RMSE lies in the range of view centimeters. To quantify the certainty of the result, the standard deviation of the individual target poses is also calculated. In this, for a given extrinsic calibration the poses of the detected calibration target are projected from the coordinate frame of the source sensor into the coordinate frame of the reference sensor. The standard deviation in the translational and rotational difference should give us a good indication on the consistency of the calibration.","title":"Data Processing"},{"location":"data_processing/#data-processing","text":"","title":"Data Processing"},{"location":"data_processing/#calibration-target-detection-in-the-camera-data","text":"The calibration target is detected in the image data by means of the ArUco markers. In this, the functionalities provided by OpenCV are utilized to detect markers and compute the pose of the calibration target with respect to the camera. To deduce the pose of the target from the marker detection, the algorithm requires the intrinsic parameters of the camera, which is why a good intrinsic calibration is vital. The image points of the detected markers are stored to be later used as 2D correspondences as part of the PnP algorithm. Furthermore, with the image positions of the detected markers and the a priori knowledge about the calibration board geometry, a 3D point cloud of the calibration target is also created. The detections, as well as the 6-DOF Pose of the target and the reconstructed 3D point cloud are published after the detection was successful. In this, the marker ID to which the detected corner belongs is stored in the intensity value of the point.","title":"Calibration Target Detection in the Camera Data"},{"location":"data_processing/#calibration-target-detection-in-the-lidar-data","text":"The detection and pose estimation of the calibration target within the LiDAR cloud data is realized in a number of consecutive steps. In this numerous algorithms from the Point Cloud Library (PCL) are used. First, the input point cloud is segmented into planer regions using a region growing algorithm. This algorithm requires the point cloud to have normal vectors which are computed prior to the region growing. After that each segmented cluster whether its size and aspect ratio fits to the geometry of the calibration target. If so, the cluster is considered as region of interest (which are also published as preview) in which a detailed detection and pose estimation of the target is performed. In this, the algorithm fits a rotating bounding box to the cluster and checks its dimension. When the actual target detection and pose estimation is to be performed, the orientation of the bounding box is first aligned to the actual orientation of the calibration target using the arrangement of the cutouts. In this, all possible orientations are tested by counting the points in the point cloud at the assumed location of the cutouts. The orientation for which the smallest number of points have been accumulated is taken as the orientation of the target. This pose is used as an initialization to a RANSAC algorithm which tries to fit a custom sample consensus (SAC) model of the calibration target to the segmented region of interest. In this it computes the normal vector of the board based on a random set of points from the cluster and varies position and rotation of the target pose with each RANSAC iteration. Finally the pose with the most inliers is chosen as the winner-takes-it-all solution. As inliers all point that lie on the calibration target are chosen. With each point that falls into the circular cutout, the current estimation is penalized by reducing the number of inliers by a certain factor. When RANSAC has found a pose, the coefficients are optimized by fitting a CAD model into the point cloud using GICP. In this only the points forming the convex hull of the cloud segmented from the input cloud are used. Finally, the 3D coordinates of the marker corners are deduced based on the estimated board pose and stored as 3D correspondences. The detections, as well as the 6-DOF Pose of the target and the segmented 3D point cloud are published after the detection was successful. In this, the marker ID to which the detected corner belongs is stored in the intensity value of the point. The parameters for the algorithm outlined above are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: Input Filter: max_range : Maximum range at which to filter the incoming point cloud prior to any processing. Any point with a range (absolute distance from sensor) larger than max_range will be discarded. Turn to 0 to switch off. Normal Estimation: normal_estimation_search_method : Select method to use for neighbor search. (0 = RadiusSearch, 1 = NearestNeighborSearch) normal_estimation_search_radius : Radius in which to search for neighbors. In case of 'RadiusSearch', this is a spatial extend. In case of 'NearestNeighborSearch', this represents the number of nearest neighbors (truncated to int). Region Growing: region_growing_cluster_size_min : Minimum number of points a cluster needs to contain in order to be considered as valid inside the region growing. region_growing_cluster_size_max : Maximum number of points a cluster needs to contain in order to be considered as valid inside the region growing. region_growing_number_neighbors : Number of neighbor points to consider during region growing. region_growing_angle_thresh : Angle in degrees used as the allowable range for the normals deviation. If the deviation between points normals is less than the smoothness threshold then they are suggested to be in the same cluster. region_growing_curvature_thresh : Second criteria for the region growing. If two points have a small normals deviation then the disparity between their curvatures is tested. Size Filter: size_filter_width_min_tolerance : Tolerance (in m) of the minimum board width when filtering the clusters based on their size. size_filter_width_max_tolerance : Tolerance (in m) of the maximum board width when filtering the clusters based on their size. size_filter_height_min_tolerance : Tolerance (in m) of the minimum board height when filtering the clusters based on their size. size_filter_height_max_tolerance : Tolerance (in m) of the maximum board height when filtering the clusters based on their size. RANSAC: ransac_distance_thresh : Distance threshold (in m) from model for points to count as inliers during RANSAC. ransac_rotation_variance : Maximum angle in rotation (in degrees) to be sampled when computing the new coefficients within RANSAC. ransac_translation_variance : Maximum distance in translation (in m) to be sampled when computing the new coefficients within RANSAC. ransac_optimize_coefficients : Option to activate the optimization of the coefficients by means of ICP. target_icp_variant : Select ICP variant to use to optimize coefficients. (0 = ICP, 1 = PlaneICP, 2 = GICP) target_icp_max_correspondence_distance : Maximum distance for ICP to search for point correspondences. Given as ratio with respect to shorter side of calibration target. target_icp_rotation_tolerance : Rotation tolerance for convergence check. Given in degrees. target_icp_translation_tolerance : Translation tolerance for convergence check. Given in unit of the LiDAR point cloud, typically meters.","title":"Calibration Target Detection in the LiDAR Data"},{"location":"data_processing/#perspective-n-point-for-2d-3d-pose-estimation","text":"In the extrinsic pose estimation using sets of 2D-3D correspondences (e.g. Camera-LiDAR calibration) the Perspective-n-Point (PnP) algorithm from OpenCV is used. The parameters to configure the PnP algorithm are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: limit_single_board_rpj_error : Use max maximum reprojection error to accept during calibration of a single target pose. If false, 'board_max_rpj_error' is ignored. single_board_max_rpj_error : Limit for maximum reprojection error to accept during calibration of a single target pose. All calibrated poses, that exceed this limit are rejected. single_board_min_inliers : Threshold for minimum number of inliers to accept during calibration of a single target pose. All calibrated poses, that do not reach this threshold are rejected. pnp_inlier_rpj_error_limit : Limit for maximum reprojection error for which points are considered as RANSAC inliers during PnP.","title":"Perspective-n-Point for 2D-3D Pose Estimation"},{"location":"data_processing/#gicp-for-3d-3d-alignment","text":"In the alignment of 3D point clouds of two LiDAR sensors (e.g. LiDAR-LiDAR calibration) the Generalized-ICP (GICP) of 'small_gicp' is used. The parameters to configure the GICP algorithm are exposed as dynamic parameters and can be adjusted by calling rqt_reconfigure or opening the preferences from the UI ( Edit->Preferences ). An overview of these parameters is given blow: registration_icp_variant : Select ICP variant to use for registration. (0 = ICP, 1 = PlaneICP, 2 = GICP) registration_icp_max_correspondence_distance : Maximum distance for ICP to search for point correspondences. Given as ratio with respect to shorter side of calibration target. registration_icp_rotation_tolerance : Rotation tolerance for convergence check. Given in degrees. registration_icp_translation_tolerance : Translation tolerance for convergence check. Given in unit of the LiDAR point cloud, typically meters.","title":"GICP for 3D-3D Alignment"},{"location":"data_processing/#error-computation-and-estimation-of-calibration-certainty","text":"In the calibration using the PnP algorithm to align two sensors by means of 2D-3D correspondences, the quality of the calibration is measured by the mean reprojection error .html). For a good calibration a typical reprojection error lies between 2-4 Pixels. In the calibration of two 3D sensors in which the sensor data gets aligned by means of GIPC, the quality of the calibration is measured using the root mean squared error (RMSE) . For a good calibration a typical RMSE lies in the range of view centimeters. To quantify the certainty of the result, the standard deviation of the individual target poses is also calculated. In this, for a given extrinsic calibration the poses of the detected calibration target are projected from the coordinate frame of the source sensor into the coordinate frame of the reference sensor. The standard deviation in the translational and rotational difference should give us a good indication on the consistency of the calibration.","title":"Error Computation and Estimation of Calibration Certainty"},{"location":"installation/","text":"Installation \u00b6 The Multi-Sensor Calibration Toolbox is released as an official package for ROS 2 , named multisensor_calibration . Since ROS 1 is soon end-of-life, there will be no official release for ROS 1. In addition to the official release, multisensor_calibration can also be build from source for ROS 1 and ROS 2 as described below. Official ROS 2 Package \u00b6 sudo apt install ros-$ROS_DISTRO-multisensor-calibration Build from Source \u00b6 ROS 1 \u00b6 Initialize catkin workspace, if required: mkdir -p calibration_ws/src catkin init --workspace calibration_ws Clone repository and checkout `noetic` branch: cd calibration_ws/src git clone https://github.com/FraunhoferIOSB/multisensor_calibration.git cd multisensor_calibration git checkout noetic cd ../../ Initialize `rosdep` and install dependencies: sudo rosdep init rosdep update rosdep install --from-paths src -y --ignore-src (OPTIONAL) Copy custom example robot workspaces and populate resource file (see Workspace) page) src/multisensor_calibration/scripts/populate_robot_workspaces_qrc.sh <custom_robot_ws_directory> (OPTIONAL) Copy custom launch files src/multisensor_calibration/scripts/copy_launch_files.sh <custom_launch_directory> Run `catkin` to build from source: To build in 'Debug' mode add `-DCMAKE_BUILD_TYPE=Debug` to catkin command. If 'CMAKE_BUILD_TYPE' omitted, multisensor_calibration will be build in 'Release' mode. catkin build -j8 -DCMAKE_BUILD_TYPE=Release multisensor_calibration ROS 2 \u00b6 Create workspace folder, if required: mkdir -p calibration_ws/src Clone repository: cd calibration_ws/src git clone https://github.com/FraunhoferIOSB/multisensor_calibration.git Initialize `rosdep` and install dependencies: sudo rosdep init rosdep update rosdep install --from-paths src -y --ignore-src (OPTIONAL) Copy custom example robot workspaces and populate resource file (see Workspace page) src/multisensor_calibration/scripts/populate_robot_workspaces_qrc.sh <custom_robot_ws_directory> (OPTIONAL) Copy custom launch files src/multisensor_calibration/scripts/copy_launch_files.sh <custom_launch_directory> Run 'colcon' to build from source: To build in 'Debug' mode add '--cmake-args -DCMAKE_BUILD_TYPE=Debug' to colcon command. If 'CMAKE_BUILD_TYPE' omitted, multisensor_calibration will be build in 'Release' mode. colcon build --symlink-install --packages-up-to multisensor_calibration Requirements \u00b6 Apart from the basic catkin requirements, multisensor_calibration depends on the following third party libraries. PCL OpenCV Qt small_gicp : The package small_gicp_vendor wraps this library to make it available for other ROS components. This is a common practice for third-party libraries without available deb sources. It is licensed under the MIT-License. OpenMP (optional): This is used to parallelize and speed up the processing of each point in the point cloud. If not found by CMake the processing will be done sequentially. Doxygen (optional): If available, this Doxygen documentation will be build automatically.","title":"Installation"},{"location":"installation/#installation","text":"The Multi-Sensor Calibration Toolbox is released as an official package for ROS 2 , named multisensor_calibration . Since ROS 1 is soon end-of-life, there will be no official release for ROS 1. In addition to the official release, multisensor_calibration can also be build from source for ROS 1 and ROS 2 as described below.","title":"Installation"},{"location":"installation/#official-ros-2-package","text":"sudo apt install ros-$ROS_DISTRO-multisensor-calibration","title":"Official ROS 2 Package"},{"location":"installation/#build-from-source","text":"","title":"Build from Source"},{"location":"installation/#ros-1","text":"Initialize catkin workspace, if required: mkdir -p calibration_ws/src catkin init --workspace calibration_ws Clone repository and checkout `noetic` branch: cd calibration_ws/src git clone https://github.com/FraunhoferIOSB/multisensor_calibration.git cd multisensor_calibration git checkout noetic cd ../../ Initialize `rosdep` and install dependencies: sudo rosdep init rosdep update rosdep install --from-paths src -y --ignore-src (OPTIONAL) Copy custom example robot workspaces and populate resource file (see Workspace) page) src/multisensor_calibration/scripts/populate_robot_workspaces_qrc.sh <custom_robot_ws_directory> (OPTIONAL) Copy custom launch files src/multisensor_calibration/scripts/copy_launch_files.sh <custom_launch_directory> Run `catkin` to build from source: To build in 'Debug' mode add `-DCMAKE_BUILD_TYPE=Debug` to catkin command. If 'CMAKE_BUILD_TYPE' omitted, multisensor_calibration will be build in 'Release' mode. catkin build -j8 -DCMAKE_BUILD_TYPE=Release multisensor_calibration","title":"ROS 1"},{"location":"installation/#ros-2","text":"Create workspace folder, if required: mkdir -p calibration_ws/src Clone repository: cd calibration_ws/src git clone https://github.com/FraunhoferIOSB/multisensor_calibration.git Initialize `rosdep` and install dependencies: sudo rosdep init rosdep update rosdep install --from-paths src -y --ignore-src (OPTIONAL) Copy custom example robot workspaces and populate resource file (see Workspace page) src/multisensor_calibration/scripts/populate_robot_workspaces_qrc.sh <custom_robot_ws_directory> (OPTIONAL) Copy custom launch files src/multisensor_calibration/scripts/copy_launch_files.sh <custom_launch_directory> Run 'colcon' to build from source: To build in 'Debug' mode add '--cmake-args -DCMAKE_BUILD_TYPE=Debug' to colcon command. If 'CMAKE_BUILD_TYPE' omitted, multisensor_calibration will be build in 'Release' mode. colcon build --symlink-install --packages-up-to multisensor_calibration","title":"ROS 2"},{"location":"installation/#requirements","text":"Apart from the basic catkin requirements, multisensor_calibration depends on the following third party libraries. PCL OpenCV Qt small_gicp : The package small_gicp_vendor wraps this library to make it available for other ROS components. This is a common practice for third-party libraries without available deb sources. It is licensed under the MIT-License. OpenMP (optional): This is used to parallelize and speed up the processing of each point in the point cloud. If not found by CMake the processing will be done sequentially. Doxygen (optional): If available, this Doxygen documentation will be build automatically.","title":"Requirements"},{"location":"license/","text":"License \u00b6 The Multi-Sensor Calibration Toolbox is licensed under the BSD 3-Clause license. Note that this text refers only to the license for Multi-Sensor Calibration Toolbox itself, independent of its thirdparty dependencies, which are separately licensed. Building the toolbox with these dependencies may affect the resulting license. BSD 3-Clause License Copyright (c) 2025, Fraunhofer IOSB Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#license","text":"The Multi-Sensor Calibration Toolbox is licensed under the BSD 3-Clause license. Note that this text refers only to the license for Multi-Sensor Calibration Toolbox itself, independent of its thirdparty dependencies, which are separately licensed. Building the toolbox with these dependencies may affect the resulting license. BSD 3-Clause License Copyright (c) 2025, Fraunhofer IOSB Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"nodes_and_nodelets/","text":"Nodes, Composable Nodes and Nodelets \u00b6 Nodes \u00b6 multisensor_calibration \u00b6 Main and user-friendly entry point to start a multi-sensor calibration. This will start the Calibration Configurator with which any calibration can be parameterized and started. This does not require any launch parameters and can just be started like this: ros2 run multisensor_calibration multisensor_calibration extrinsic_camera_lidar_calibration \u00b6 The node for the guided extrinsic calibration between a camera and a LiDAR sensor with a user-friendly UI. It comprises the ExtrinsicCameraLidarCalibration(Nodelet) performing the actual calibration, the GuidedCameraLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'CameraLidarCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String lidar_sensor_name : Name of the LiDAR sensor with respect to which the camera is to be calibrated. Type: String lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false extrinsic_lidar_lidar_calibration \u00b6 The node for the guided extrinsic calibration between two LiDAR sensors with a user-friendly UI. It comprises the ExtrinsicLidarLidarCalibration(Nodelet) performing the actual calibration, the GuidedLidarLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'LidarLidarCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_sensor_name : ame of the reference LiDAR sensor with respect to which the source LiDAR sensor is to be calibrated. Type: String ref_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false align_ground_planes : Set to true, to additionally align the ground planes in the sensor data. Additionally specify the upright frame ID. Type: bool Default: false upright_frame_id : ID of Frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false extrinsic_camera_reference_calibration \u00b6 The node for the guided extrinsic calibration of a camera with respect to a reference with a user-friendly UI. It comprises the ExtrinsicCameraReferenceCalibration(Nodelet) performing the actual calibration, the GuidedCameraLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'CameraReferenceCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false extrinsic_lidar_reference_calibration \u00b6 The node for the guided extrinsic calibration of a LiDAR sensor with respect to a reference with a user-friendly UI. It comprises the ExtrinsicLidarReferenceCalibration(Nodelet) performing the actual calibration, the GuidedLidarLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'LidarReferenceCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false publish_pointcloud \u00b6 Node to load a point cloud from a specified PLY file and publish it on the given topic with the given frame_id. *This is helpful to publish the model cloud in the context of extrinsic LiDAR-vehicle calibration . Launch-Parameters: point_cloud_file : PLY file from which to load the point cloud. Type: String Default: \"\" topic_name : Topic name on which to publish the point cloud. Type: String Default: \"\" frame_id : Frame ID at which to publish the point cloud. Type: String Default: \"\" initialize_robot_workspace \u00b6 Node to initialize a non-existing robot workspace with given information. Launch-Parameters: robot_ws_path : Path to where the robot workspace is to be initialized. Type: String Default: \"\" robot_name : Name of the robot to which the workspace corresponds. Type: String Default: \"\" urdf_model_path : (Optional) Path to URDF model associated with the robot. Type: String Default: \"\" Composable Nodes / Nodelets \u00b6 CameraTargetDetection / CameraDataProcessingNodelet \u00b6 Nodelet to run the processing of the camera data and, in turn, the detection of the calibration target within the camera data isolated from the rest. This is particularly helpful to develop and debug the detection of the calibration target within the camera data. Launch-Parameters: camera : Namespace of the camera. Type: String Default: \"/camera\" image : Name of the image topic within the camera namespace. Type: String Default: \"image_color\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Topics Published: ~/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the intensity field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. Services Advertised: ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/capture_target : Service to trigger the capturing of the target. ~/request_processor_state : Service to get initialization state of the processor. LidarTargetDetection / LidarDataProcessingNodelet \u00b6 Nodelet to run the processing of the LiDAR data and, in turn, the detection of the calibration target within the LiDAR point clouds isolated from the rest. This is particularly helpful to develop and debug the detection of the calibration target within the camera data. Launch-Parameters: cloud : Topic name of the LiDAR cloud messages in which the target is to be detected. Type: String Default: \"/cloud\"* target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Topics Published: ~/regions_of_interest : Cloud holding separate regions of the input sensor cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/target_pattern : Cloud holding points of the calibration target detected in the LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. Services Advertised: ~/capture_target : Service to trigger the capturing of the target. ~/request_processor_state : Service to get initialization state of the processor. ~/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker. This is not relevant for this nodelet. ~/import_marker_observations : Service to import a set of marker observations. This is not relevant for this nodelet. ExtrinsicCameraLidarCalibration(Nodelet) \u00b6 Nodelet to perform extrinsic camera-LiDAR calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously detects the ArUco marker inside the camera image and selects possible region of interests inside the LiDAR cloud. It publishes the marker detections as well as the regions of interest in the topic '~/<camera_sensor_name>/annotated_image' and '~/<lidar_sensor_name>/regions_of_interest', respectively. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected in both the camera image as well as the LiDAR data, an instant calibration is performed with just this target pose. A corresponding message is printed in the command line with the mean reprojection error of this calibration. If not successful a corresponding message is also printed. Typical reasons for a detection not being successful is either that the target is not detected in the LiDAR scan or that the reprojection error exceeds a certain threshold (Default: 2px). If latter is the case the threshold can be increased by calling 'rqt_dynamic_reconfigure'. Since the detection of the calibration target from the camera image is very reliable, only the detection inside the LiDAR cloud is published for visual inspection, i.e. inside the topics '~/<lidar_sensor_name>/target_pattern' and '~/<lidar_sensor_name>/marker_corners'. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. For a good calibration 5 to 7 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-7. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall mean reprojection error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. NOTE: For a good final calibration a typical mean reprojection error is between 2-4 pixels. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String lidar_sensor_name : Name of the LiDAR sensor with respect to which the camera is to be calibrated. Type: String lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false Topics Published: ~/<camera_sensor_name>/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/<camera_sensor_name>/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the input sensor cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/<lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor. ExtrinsicLidarLidarCalibration(Nodelet) \u00b6 Nodelet to perform extrinsic LiDAR-LiDAR calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously selects possible region of interests inside both LiDAR clouds. It publishes the marker detections as well as the regions of interest in the topics '~/<[src|ref]_lidar_sensor_name>/regions_of_interest'. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected in both LiDAR data, an instant calibration is performed with just this target pose. A corresponding message is printed in the command line with the mean reprojection error of this calibration. If not successful a corresponding message is also printed. Typical reasons for a detection not being successful is that the target is not detected in the LiDAR scan. The detection inside the LiDAR cloud is published for visual inspection, i.e. inside the topics '~/<[src|ref]_lidar_sensor_name>/target_pattern' and '~/<[src|ref]_lidar_sensor_name>/marker_corners'. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. For a good calibration 3 to 5 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-7. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall root mean squared error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_sensor_name : ame of the reference LiDAR sensor with respect to which the source LiDAR sensor is to be calibrated. Type: String ref_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false align_ground_planes : Set to true, to additionally align the ground planes in the sensor data. Additionally specify the upright frame ID. Type: bool Default: false upright_frame_id : ID of Frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the source LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<src_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<ref_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the input reference LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<ref_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the reference LiDAR cloud. This is only available after the calibration target has been detected in the reference LiDAR cloud. ~/<ref_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the reference LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<ref_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/<src_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the source LiDAR data. ~/<src_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the source LiDAR data. This will remove all previous observations captured for this sensor. ~/<ref_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the reference LiDAR data. ~/<ref_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the reference LiDAR data. This will remove all previous observations captured for this sensor. ExtrinsicCameraReferenceCalibration(Nodelet) \u00b6 Nodelet to perform extrinsic camera-reference calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously detects the ArUco marker inside the camera image. It publishes the marker detections in the topic '~/<camera_sensor_name>/annotated_image', respectively. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose in the camera image, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected a corresponding message is printed in the command line. If not successful a corresponding message is also printed. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. Measure the top-left corner of each ArUco marker, for example, with a Total Station. Pass all measured coordinates of the top-left marker corners to the calibration by calling 'ros2 service call /<node_name>/ /add_marker_observations iosb_calibration_interface/srv/AddMarkerObservations \"{ observation: { marker_ids: [1, 2, 3, 4], marker_top_left_point: [{x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}]} }'. For a good calibration 5 to 7 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-8. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall mean reprojection error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. For a good final calibration a typical mean reprojection error is between 2-4 pixels. NOTE: Step 8 (entering of the reference coordinates) does not necessarily need to be done directly after each measuring of the coordinates. It is can also be done iteratively for all observations before Step 10. However, it is important that the measurements are added in the correct order. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<camera_sensor_name>/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/<camera_sensor_name>/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/<reference_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<reference_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor. ExtrinsicLidarReferenceCalibration(Nodelet) \u00b6 Nodelet to perform extrinsic lidar-reference calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously selects possible region of interests inside the LiDAR cloud. It publishes the marker detections as well as the regions of interest in the topics '~/ /regions_of_interest'. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose in the camera image, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected a corresponding message is printed in the command line. If not successful a corresponding message is also printed. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. Measure the top-left corner of each ArUco marker, for example, with a Total Station. Pass all measured coordinates of the top-left marker corners to the calibration by calling 'ros2 service call /<node_name>/ /add_marker_observations iosb_calibration_interface/srv/AddMarkerObservations \"{ observation: { marker_ids: [1, 2, 3, 4], marker_top_left_point: [{x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}]} }'. For a good calibration 3 to 5 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-8. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall root mean squared error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. NOTE: Step 8 (entering of the reference coordinates) does not necessarily need to be done directly after each measuring of the coordinates. It is can also be done iteratively for all observations before Step 10. However, it is important that the measurements are added in the correct order. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the source LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<src_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/<src_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the source LiDAR data. ~/<src_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the source LiDAR data. This will remove all previous observations captured for this sensor. ~/<reference_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<reference_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor. ExtrinsicLidarVehicleCalibration(Nodelet) \u00b6 Nodelet to perform extrinsic LiDAR-LiDAR calibration without any ui. The idea in this calibration is that for which have LiDAR sensors that also \"see\" part of the robots body, on can estimate the position of the LiDAR sensor by aligning the LiDAR scans to a model of the robots body. This is done by manually selecting corresponding regions in both sensor cloud and model. To do so, use the point picking tool of rviz to pick a point in the corresponding clouds round which a planar region is to be estimated. The selected regions are then aligned using a GICP. For region selection a plane is fitted into the cloud at a selected marker point using RANSAC. Prior to GICP the clouds are filtered using Statistical Outlier Removal. The transformation guess for the GICP is computed by using a Levenberg-Marquardt algorithm to estimate a rigid transformation from the seed points around the selected region markers. NOTE: This work, however, is a prototype implementation and needs further development. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_cloud_topic : Topic name of the reference LiDAR cloud from CAD model. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding regions of the source sensor which are used to align the sensor cloud to the cloud of the vehicle model. These are computed based on the seed points clicked. ~/<ref_lidar_sensor_name>/regions_of_interest : Cloud holding regions of the source sensor which are used to align the sensor cloud to the cloud of the vehicle model. These are computed based on the seed points clicked. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/add_region_marker : Service to a new seed point for a region which is later to be used to align the sensor cloud to that of the vehicle cad. Use the frame ID to distinguish between sensor cloud and vehicle cloud. Subscribed Topics: clicked_point : Message topic provided by rviz when a point is clicked. GuidedCameraLidarTargetPlacement(Nodelet) \u00b6 Nodelet for guided target placement in the context of extrinsic camera-lidar calibration. NOTE: The guidance feature of multisensor_calibration is currently still in development and not yet fully functional. GuidedLidarLidarTargetPlacement(Nodelet) \u00b6 Nodelet for guided target placement in the context of extrinsic lidar-lidar calibration. NOTE: The guidance feature of multisensor_calibration is currently still in development and not yet fully functional. PointCloud2Image(Nodelet) \u00b6 Nodelet to fuse a point cloud and a camera image by projecting the geometric 3D information from the point cloud into the camera image. In this, the projected points from the point cloud are colorized based on their depth, i.e. distance from the camera. The colorization is performed by applying the RAINBOW colormap from cv::ColorMap . This colorizes the range from min_depth to max_depth going from red to violet. Launch-Parameters: image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. camera_namespace : Optional parameter to provide a separate camera namespace from which the topic name of the camera info can be constructed. Type: String Default: \"\" min_depth : Minimum depth for which to project points from the point cloud into the image. All points that are closer to the camera than the specified min. depth will be discarded. Type: float Default: 0.1 max_depth : Maximum depth for which to project points from the point cloud into the image. All points that are further away from the camera than the specified max. depth will be discarded. Type: float Default: 20.0 ``sync_queue_size : Queue size for the data synchronization using the method approximated time synchronization. Not evaluated when using exact time synchronization. Type: int Default: 100 use_exact_sync : Flag to activate method for exact time synchronization. Type: bool Default: false* temp_transform : Temporary transform of camera with respect to LiDAR given in the form of XYZ and RPY. The values are to be given as string list separated by whitespace. If left empty or omitted the transformation will be extracted from the TF-tree by using the frame IDs of the images and the clouds. Type: String Default: \"\" Subscribed Topics: pointcloud : Subscribes to point cloud that is to be projected into the camera image. Type: sensor_msgs::PointCloud2 image : Subscribes to RGB input image from which to draw the pixel information to enhance the point cloud with color information. Type: sensor_msgs::Image Published Topics: ~/fused_image : Publishes the resulting RGB image in which the input camera vis image is fused with the input point cloud. The projected points from the point cloud are colorized based on their depth, i.e. distance from the camera. Type: sensor_msgs::Image PointCloud2PointCloudDistance(Nodelet) \u00b6 Nodelet to calculate the n-to-n distance for a list of point clouds. For each point cloud in the list, the smallest distance with respect to all other point clouds is found. The calculated distance is normalized with respect to a given maximum distance and stored in the intensity field of the source point cloud. This can then be used to colorize the distance in a point cloud viewer, such as rviz. To calculate the distance between the point clouds, this nodelet uses two distance measures, which can be selected by the appropriate launch parameter: point_2_point : This calculates the euclidean distance between each point in the source point cloud and the nearest neighbor in the target point cloud (transformed into the frame of the source point cloud). In this, it finds the n nearest neighbors (ordered by their distance) and takes the distance to the first one in the list. point_2_surface : In order to calculate the distance of a point to a surface, information on the surface normal needs be available. Thus, in this case, the nodelet first computes the normal vectors for the transformed target point cloud. It then, again, finds the n nearest neighbors in the target point cloud to each point in the source cloud. The distance is then calculated by first computing the plane parameterization for each neighbor and its corresponding normal vector. This followed by calculating the orthogonal distances of the source point w.r.t. each parameterized surface. The smallest of these orthogonal distances is then selected as the point-2-surface distance. Prior to calculating the distance between two point clouds, the target point cloud is transformed into the coordinate system of the source point cloud. In this way, since the source cloud is to be enhanced with distance information, the source cloud does not need to be transformed back prior to publishing. The transformation is found, by first extracting the frame IDs from header of each point cloud, which can then be used to look it up in the TF tree. Launch-Parameters: number_of_clouds : Number of clouds to process. The input topics are appended with a index as suffix: cloud_0 , cloud_1 , ..., cloud_N-1 . Type: int Default: 2 processing_rate : Rate in (Hz) at which to process the received point cloud. Type: int Default: 1 distance_measure : Enum: {0 = point_2_point , 1 = point_2_surface } Type: Enum: {0 = point_2_point, 1 = point_2_surface} Default: 0 NOTE: The point_2_surface measure is more robust with a higher number of nearest neighbors. Especially when using point clouds from a rotating LiDAR, as a parameterization of a small number of neighbors might lead to a selection of all neighbors on one ring, making the surface estimation less accurate. num_nearest_neighbors : Number of nearest neighbors to consider in the target cloud for each point in the source cloud. Type: int Default: 5 max_distance : Maximum distance at which to truncate. This is also used for normalization when calculating the intensity. Type: float Default: 5.0 clamp_distance_threshold : Distance at which to clamp the source point cloud. Points exceeding the distance are removed. It is truncated to a minimum of 'max_distance'. Type: float Default: FLT_MAX temp_transform : (Only available if 'number_of_clouds = 2') Temporary transform of camera with respect to LiDAR given in the form of XYZ and RPY. The values are to be given as string list separated by whitespace. If left empty or omitted the transformation will be extracted from the TF-tree by using the frame IDs of the images and the clouds. Type: String Default: \"\" Subscribed Topics: cloud_<idx> : Point cloud for which the distance is to be calculated. Type: sensor_msgs::PointCloud2 Published Topics: ~/cloud_<idx>_enhanced : Source point cloud enhanced with the point-wise normalized distance (with the range of [0,1]) to the target point cloud in the intensity field. Type: sensor_msgs::PointCloud2","title":"Nodes and Nodelets"},{"location":"nodes_and_nodelets/#nodes-composable-nodes-and-nodelets","text":"","title":"Nodes, Composable Nodes and Nodelets"},{"location":"nodes_and_nodelets/#nodes","text":"","title":"Nodes"},{"location":"nodes_and_nodelets/#multisensor_calibration","text":"Main and user-friendly entry point to start a multi-sensor calibration. This will start the Calibration Configurator with which any calibration can be parameterized and started. This does not require any launch parameters and can just be started like this: ros2 run multisensor_calibration multisensor_calibration","title":"multisensor_calibration"},{"location":"nodes_and_nodelets/#extrinsic_camera_lidar_calibration","text":"The node for the guided extrinsic calibration between a camera and a LiDAR sensor with a user-friendly UI. It comprises the ExtrinsicCameraLidarCalibration(Nodelet) performing the actual calibration, the GuidedCameraLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'CameraLidarCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String lidar_sensor_name : Name of the LiDAR sensor with respect to which the camera is to be calibrated. Type: String lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false","title":"extrinsic_camera_lidar_calibration"},{"location":"nodes_and_nodelets/#extrinsic_lidar_lidar_calibration","text":"The node for the guided extrinsic calibration between two LiDAR sensors with a user-friendly UI. It comprises the ExtrinsicLidarLidarCalibration(Nodelet) performing the actual calibration, the GuidedLidarLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'LidarLidarCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_sensor_name : ame of the reference LiDAR sensor with respect to which the source LiDAR sensor is to be calibrated. Type: String ref_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false align_ground_planes : Set to true, to additionally align the ground planes in the sensor data. Additionally specify the upright frame ID. Type: bool Default: false upright_frame_id : ID of Frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false","title":"extrinsic_lidar_lidar_calibration"},{"location":"nodes_and_nodelets/#extrinsic_camera_reference_calibration","text":"The node for the guided extrinsic calibration of a camera with respect to a reference with a user-friendly UI. It comprises the ExtrinsicCameraReferenceCalibration(Nodelet) performing the actual calibration, the GuidedCameraLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'CameraReferenceCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false","title":"extrinsic_camera_reference_calibration"},{"location":"nodes_and_nodelets/#extrinsic_lidar_reference_calibration","text":"The node for the guided extrinsic calibration of a LiDAR sensor with respect to a reference with a user-friendly UI. It comprises the ExtrinsicLidarReferenceCalibration(Nodelet) performing the actual calibration, the GuidedLidarLidarTargetPlacement(Nodelet) responsible for guiding the user to place the calibration target, as well as an instance of 'LidarReferenceCalibrationGui' as a graphical user interface. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false","title":"extrinsic_lidar_reference_calibration"},{"location":"nodes_and_nodelets/#publish_pointcloud","text":"Node to load a point cloud from a specified PLY file and publish it on the given topic with the given frame_id. *This is helpful to publish the model cloud in the context of extrinsic LiDAR-vehicle calibration . Launch-Parameters: point_cloud_file : PLY file from which to load the point cloud. Type: String Default: \"\" topic_name : Topic name on which to publish the point cloud. Type: String Default: \"\" frame_id : Frame ID at which to publish the point cloud. Type: String Default: \"\"","title":"publish_pointcloud"},{"location":"nodes_and_nodelets/#initialize_robot_workspace","text":"Node to initialize a non-existing robot workspace with given information. Launch-Parameters: robot_ws_path : Path to where the robot workspace is to be initialized. Type: String Default: \"\" robot_name : Name of the robot to which the workspace corresponds. Type: String Default: \"\" urdf_model_path : (Optional) Path to URDF model associated with the robot. Type: String Default: \"\"","title":"initialize_robot_workspace"},{"location":"nodes_and_nodelets/#composable-nodes-nodelets","text":"","title":"Composable Nodes / Nodelets"},{"location":"nodes_and_nodelets/#cameratargetdetection-cameradataprocessingnodelet","text":"Nodelet to run the processing of the camera data and, in turn, the detection of the calibration target within the camera data isolated from the rest. This is particularly helpful to develop and debug the detection of the calibration target within the camera data. Launch-Parameters: camera : Namespace of the camera. Type: String Default: \"/camera\" image : Name of the image topic within the camera namespace. Type: String Default: \"image_color\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Topics Published: ~/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the intensity field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. Services Advertised: ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/capture_target : Service to trigger the capturing of the target. ~/request_processor_state : Service to get initialization state of the processor.","title":"CameraTargetDetection / CameraDataProcessingNodelet"},{"location":"nodes_and_nodelets/#lidartargetdetection-lidardataprocessingnodelet","text":"Nodelet to run the processing of the LiDAR data and, in turn, the detection of the calibration target within the LiDAR point clouds isolated from the rest. This is particularly helpful to develop and debug the detection of the calibration target within the camera data. Launch-Parameters: cloud : Topic name of the LiDAR cloud messages in which the target is to be detected. Type: String Default: \"/cloud\"* target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Topics Published: ~/regions_of_interest : Cloud holding separate regions of the input sensor cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/target_pattern : Cloud holding points of the calibration target detected in the LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. Services Advertised: ~/capture_target : Service to trigger the capturing of the target. ~/request_processor_state : Service to get initialization state of the processor. ~/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker. This is not relevant for this nodelet. ~/import_marker_observations : Service to import a set of marker observations. This is not relevant for this nodelet.","title":"LidarTargetDetection / LidarDataProcessingNodelet"},{"location":"nodes_and_nodelets/#extrinsiccameralidarcalibrationnodelet","text":"Nodelet to perform extrinsic camera-LiDAR calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously detects the ArUco marker inside the camera image and selects possible region of interests inside the LiDAR cloud. It publishes the marker detections as well as the regions of interest in the topic '~/<camera_sensor_name>/annotated_image' and '~/<lidar_sensor_name>/regions_of_interest', respectively. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected in both the camera image as well as the LiDAR data, an instant calibration is performed with just this target pose. A corresponding message is printed in the command line with the mean reprojection error of this calibration. If not successful a corresponding message is also printed. Typical reasons for a detection not being successful is either that the target is not detected in the LiDAR scan or that the reprojection error exceeds a certain threshold (Default: 2px). If latter is the case the threshold can be increased by calling 'rqt_dynamic_reconfigure'. Since the detection of the calibration target from the camera image is very reliable, only the detection inside the LiDAR cloud is published for visual inspection, i.e. inside the topics '~/<lidar_sensor_name>/target_pattern' and '~/<lidar_sensor_name>/marker_corners'. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. For a good calibration 5 to 7 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-7. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall mean reprojection error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. NOTE: For a good final calibration a typical mean reprojection error is between 2-4 pixels. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String lidar_sensor_name : Name of the LiDAR sensor with respect to which the camera is to be calibrated. Type: String lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false Topics Published: ~/<camera_sensor_name>/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/<camera_sensor_name>/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the input sensor cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/<lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor.","title":"ExtrinsicCameraLidarCalibration(Nodelet)"},{"location":"nodes_and_nodelets/#extrinsiclidarlidarcalibrationnodelet","text":"Nodelet to perform extrinsic LiDAR-LiDAR calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously selects possible region of interests inside both LiDAR clouds. It publishes the marker detections as well as the regions of interest in the topics '~/<[src|ref]_lidar_sensor_name>/regions_of_interest'. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected in both LiDAR data, an instant calibration is performed with just this target pose. A corresponding message is printed in the command line with the mean reprojection error of this calibration. If not successful a corresponding message is also printed. Typical reasons for a detection not being successful is that the target is not detected in the LiDAR scan. The detection inside the LiDAR cloud is published for visual inspection, i.e. inside the topics '~/<[src|ref]_lidar_sensor_name>/target_pattern' and '~/<[src|ref]_lidar_sensor_name>/marker_corners'. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. For a good calibration 3 to 5 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-7. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall root mean squared error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_sensor_name : ame of the reference LiDAR sensor with respect to which the source LiDAR sensor is to be calibrated. Type: String ref_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false align_ground_planes : Set to true, to additionally align the ground planes in the sensor data. Additionally specify the upright frame ID. Type: bool Default: false upright_frame_id : ID of Frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false sync_queue_size : Queue size used for the synchronization between the messages of the camera images and the LiDAR clouds Type: int Default: 100 use_exact_sync : Set to true if an exact time synchronization between the camera image messages and the LiDAR cloud messages. Type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the source LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<src_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<ref_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the input reference LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<ref_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the reference LiDAR cloud. This is only available after the calibration target has been detected in the reference LiDAR cloud. ~/<ref_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the reference LiDAR cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<ref_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/<src_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the source LiDAR data. ~/<src_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the source LiDAR data. This will remove all previous observations captured for this sensor. ~/<ref_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the reference LiDAR data. ~/<ref_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the reference LiDAR data. This will remove all previous observations captured for this sensor.","title":"ExtrinsicLidarLidarCalibration(Nodelet)"},{"location":"nodes_and_nodelets/#extrinsiccamerareferencecalibrationnodelet","text":"Nodelet to perform extrinsic camera-reference calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously detects the ArUco marker inside the camera image. It publishes the marker detections in the topic '~/<camera_sensor_name>/annotated_image', respectively. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose in the camera image, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected a corresponding message is printed in the command line. If not successful a corresponding message is also printed. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. Measure the top-left corner of each ArUco marker, for example, with a Total Station. Pass all measured coordinates of the top-left marker corners to the calibration by calling 'ros2 service call /<node_name>/ /add_marker_observations iosb_calibration_interface/srv/AddMarkerObservations \"{ observation: { marker_ids: [1, 2, 3, 4], marker_top_left_point: [{x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}]} }'. For a good calibration 5 to 7 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-8. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall mean reprojection error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. For a good final calibration a typical mean reprojection error is between 2-4 pixels. NOTE: Step 8 (entering of the reference coordinates) does not necessarily need to be done directly after each measuring of the coordinates. It is can also be done iteratively for all observations before Step 10. However, it is important that the measurements are added in the correct order. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String camera_sensor_name : Name of the camera sensor that is to be calibrated. Type: String camera_image_topic : Topic name of the corresponding camera images. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String camera_info_topic : Name of the camera info topic. If this parameter is omitted the camera info topic name is constructed from the specified camera_image_topic . Type: String Default: \"\" image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. is_stereo_camera : Set to true, if camera is to be calibrated as stereo camera. If set to true, 'right_camera_sensor_name' and 'right_camera_info_topic' also need to be set. Type: bool Default: false right_camera_sensor_name : Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" right_camera_info_topic : Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Required if is_stereo_camera == true . Type: String Default: \"\" rect_suffix : Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'image_state' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the imageState_ is STEREO_RECTIFIED this is removed from the frame id. Type: String Default: \"_rect\" base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<camera_sensor_name>/annotated_image : Camera image, which is annotated with the detected markers of the calibration target. ~/<camera_sensor_name>/target_pattern : Cloud of the calibration target detected in the camera and reprojected into a 3D cloud. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of he marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/<camera_sensor_name>/board_pose : 6-DOF pose of the detected calibration target. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/request_camera_intrinsics : Service to request the loaded intrinsics of the camera sensor. ~/<reference_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<reference_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor.","title":"ExtrinsicCameraReferenceCalibration(Nodelet)"},{"location":"nodes_and_nodelets/#extrinsiclidarreferencecalibrationnodelet","text":"Nodelet to perform extrinsic lidar-reference calibration without any ui or guidance. How to use as standalone: After starting up, the calibration nodelet continuously selects possible region of interests inside the LiDAR cloud. It publishes the marker detections as well as the regions of interest in the topics '~/ /regions_of_interest'. Visualize the data published in the previous step for example in rviz and make sure that the calibration target is seen both in the camera image as well as the LiDAR scan. To trigger the detection of the calibration target and to capture its pose in the camera image, call the provided service of the calibration nodelet: '~/capture_target'. Depending on the data, the detection of the target can take up to a couple of seconds. When the target has successfully been detected a corresponding message is printed in the command line. If not successful a corresponding message is also printed. If the detected pose is not satisfactory one can remove the last detection by calling the appropriate service: '~/remove_last_observation'. Measure the top-left corner of each ArUco marker, for example, with a Total Station. Pass all measured coordinates of the top-left marker corners to the calibration by calling 'ros2 service call /<node_name>/ /add_marker_observations iosb_calibration_interface/srv/AddMarkerObservations \"{ observation: { marker_ids: [1, 2, 3, 4], marker_top_left_point: [{x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}, {x: 1.0, y: 2.0, z: 3.0}]} }'. For a good calibration 3 to 5 different poses of the calibration target should be used, ideally moving it around the three coordinate axes. For each pose, repeat steps 2-8. When enough different calibration poses are collected, the calibration can be finalized by calling the service: '~/finalize_calibration'. This will do a final calibration, print out the resulting extrinsic parameters as well as the overall root mean squared error and the confidence of the calibration. The calibration results and a urdf snippet, together with the observations (if requested) are written into the calibration workspace. NOTE: Step 8 (entering of the reference coordinates) does not necessarily need to be done directly after each measuring of the coordinates. It is can also be done iteratively for all observations before Step 10. However, it is important that the measurements are added in the correct order. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String reference_name : Name of the reference calibrated. Type: String reference_frame_id : Frame ID in which the reference data is provided. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding separate regions of the source LiDAR cloud in which the calibration target is searched for. These are the product of the first preprocessing of the sensor cloud to reduce the amount of data to be processed. ~/<src_lidar_sensor_name>/board_pose : 6-DOF pose of the detected calibration target in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/target_pattern : Cloud holding points of the calibration target detected in the source LiDAR cloud. This is only available after the calibration target has been detected in the source LiDAR cloud. ~/<src_lidar_sensor_name>/marker_corners : Corners of the ArUco markers on the calibration target deduced from the detected pose of the target. Each point of the marker corner is enhanced with the ID of the ArUco marker inside the 'intensity' field. This is only available after the calibration target has been detected in the LiDAR cloud. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/capture_target : Service to trigger the capturing of the target in both sensors. This will add an observation to the list, if the target detection was successful. ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/<src_lidar_sensor_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the source LiDAR data. ~/<src_lidar_sensor_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the source LiDAR data. This will remove all previous observations captured for this sensor. ~/<reference_name>/add_marker_observations : Service to add the coordinates of the top-left corners of the ArUco marker to the LiDAR data as reference. ~/<reference_name>/import_marker_observations : Service to import a set of the top-left corners of the ArUco marker to the LiDAR data as reference. This will remove all previous observations captured for this sensor.","title":"ExtrinsicLidarReferenceCalibration(Nodelet)"},{"location":"nodes_and_nodelets/#extrinsiclidarvehiclecalibrationnodelet","text":"Nodelet to perform extrinsic LiDAR-LiDAR calibration without any ui. The idea in this calibration is that for which have LiDAR sensors that also \"see\" part of the robots body, on can estimate the position of the LiDAR sensor by aligning the LiDAR scans to a model of the robots body. This is done by manually selecting corresponding regions in both sensor cloud and model. To do so, use the point picking tool of rviz to pick a point in the corresponding clouds round which a planar region is to be estimated. The selected regions are then aligned using a GICP. For region selection a plane is fitted into the cloud at a selected marker point using RANSAC. Prior to GICP the clouds are filtered using Statistical Outlier Removal. The transformation guess for the GICP is computed by using a Levenberg-Marquardt algorithm to estimate a rigid transformation from the seed points around the selected region markers. NOTE: This work, however, is a prototype implementation and needs further development. Launch-Parameters: robot_ws_path : Path to the folder holding the robot workspace. This will NOT be created if it does not yet exist. See section 'Initialize new Robot workspace' to find out how to create a new one Type: String target_config_file : Path to the file holding the configuration of the calibration target . E.g. \"$(find multisensor_calibration)/config/TargetWithCirclesAndAruco.yaml\" Type: String src_lidar_sensor_name : Name of the source lidar sensor that is to be calibrated. Type: String src_lidar_cloud_topic : Topic name of the corresponding LiDAR cloud. Type: String ref_lidar_cloud_topic : Topic name of the reference LiDAR cloud from CAD model. Type: String base_frame_id : If specified, the extrinsic pose will be calculated with respect to frame of the given frame ID. This does not change the frame ID of the reference sensor, i.e. the LiDAR sensor, but will perform an a posteriori transformation of the estimated extrinsic pose into the specified frame. If not specified, or left empty, the extrinsic pose will be calculated with respect to the frame of the reference sensor. Type: String Default: \"\" use_initial_guess : Option to use an initial guess on the extrinsic sensor pose from the TF-tree, if available. Type: bool Default: false save_observations : Option to save recorded observations that have been used for the calibration to the workspace type: bool Default: false Topics Published: ~/<src_lidar_sensor_name>/regions_of_interest : Cloud holding regions of the source sensor which are used to align the sensor cloud to the cloud of the vehicle model. These are computed based on the seed points clicked. ~/<ref_lidar_sensor_name>/regions_of_interest : Cloud holding regions of the source sensor which are used to align the sensor cloud to the cloud of the vehicle model. These are computed based on the seed points clicked. ~/calibration_result : Calibration result published after a successful calibration. Services Advertised: ~/remove_last_observation : Service to remove last observation from the top of the list. This can be executed repeatedly until the observation list is empty. ~/finalize_calibration : Service to finalize calibration, i.e. calibrate the extrinsic pose based on all captured observations. ~/reset : Service to reset the calibration. ~/request_calibration_meta_data : Service to request the meta information of the calibration, e.g. sensor names, topic names, and more. ~/request_sensor_extrinsics : Service to request the currently calculated extrinsic pose between the two sensors. ~/add_region_marker : Service to a new seed point for a region which is later to be used to align the sensor cloud to that of the vehicle cad. Use the frame ID to distinguish between sensor cloud and vehicle cloud. Subscribed Topics: clicked_point : Message topic provided by rviz when a point is clicked.","title":"ExtrinsicLidarVehicleCalibration(Nodelet)"},{"location":"nodes_and_nodelets/#guidedcameralidartargetplacementnodelet","text":"Nodelet for guided target placement in the context of extrinsic camera-lidar calibration. NOTE: The guidance feature of multisensor_calibration is currently still in development and not yet fully functional.","title":"GuidedCameraLidarTargetPlacement(Nodelet)"},{"location":"nodes_and_nodelets/#guidedlidarlidartargetplacementnodelet","text":"Nodelet for guided target placement in the context of extrinsic lidar-lidar calibration. NOTE: The guidance feature of multisensor_calibration is currently still in development and not yet fully functional.","title":"GuidedLidarLidarTargetPlacement(Nodelet)"},{"location":"nodes_and_nodelets/#pointcloud2imagenodelet","text":"Nodelet to fuse a point cloud and a camera image by projecting the geometric 3D information from the point cloud into the camera image. In this, the projected points from the point cloud are colorized based on their depth, i.e. distance from the camera. The colorization is performed by applying the RAINBOW colormap from cv::ColorMap . This colorizes the range from min_depth to max_depth going from red to violet. Launch-Parameters: image_state : State of the camera images used. Type: String Default: 'DISTORTED' Possible values: 'DISTORTED': As it comes from the camera, i.e. image distortion has not yet been corrected and it is not rectified for stereo processing. 'UNDISTORTED': Image with no distortion but not rectified for stereo processing. 'STEREO_RECTIFIED': Image rectified for stereo processing, i.e. the epipolar lines are horizontally aligned. camera_namespace : Optional parameter to provide a separate camera namespace from which the topic name of the camera info can be constructed. Type: String Default: \"\" min_depth : Minimum depth for which to project points from the point cloud into the image. All points that are closer to the camera than the specified min. depth will be discarded. Type: float Default: 0.1 max_depth : Maximum depth for which to project points from the point cloud into the image. All points that are further away from the camera than the specified max. depth will be discarded. Type: float Default: 20.0 ``sync_queue_size : Queue size for the data synchronization using the method approximated time synchronization. Not evaluated when using exact time synchronization. Type: int Default: 100 use_exact_sync : Flag to activate method for exact time synchronization. Type: bool Default: false* temp_transform : Temporary transform of camera with respect to LiDAR given in the form of XYZ and RPY. The values are to be given as string list separated by whitespace. If left empty or omitted the transformation will be extracted from the TF-tree by using the frame IDs of the images and the clouds. Type: String Default: \"\" Subscribed Topics: pointcloud : Subscribes to point cloud that is to be projected into the camera image. Type: sensor_msgs::PointCloud2 image : Subscribes to RGB input image from which to draw the pixel information to enhance the point cloud with color information. Type: sensor_msgs::Image Published Topics: ~/fused_image : Publishes the resulting RGB image in which the input camera vis image is fused with the input point cloud. The projected points from the point cloud are colorized based on their depth, i.e. distance from the camera. Type: sensor_msgs::Image","title":"PointCloud2Image(Nodelet)"},{"location":"nodes_and_nodelets/#pointcloud2pointclouddistancenodelet","text":"Nodelet to calculate the n-to-n distance for a list of point clouds. For each point cloud in the list, the smallest distance with respect to all other point clouds is found. The calculated distance is normalized with respect to a given maximum distance and stored in the intensity field of the source point cloud. This can then be used to colorize the distance in a point cloud viewer, such as rviz. To calculate the distance between the point clouds, this nodelet uses two distance measures, which can be selected by the appropriate launch parameter: point_2_point : This calculates the euclidean distance between each point in the source point cloud and the nearest neighbor in the target point cloud (transformed into the frame of the source point cloud). In this, it finds the n nearest neighbors (ordered by their distance) and takes the distance to the first one in the list. point_2_surface : In order to calculate the distance of a point to a surface, information on the surface normal needs be available. Thus, in this case, the nodelet first computes the normal vectors for the transformed target point cloud. It then, again, finds the n nearest neighbors in the target point cloud to each point in the source cloud. The distance is then calculated by first computing the plane parameterization for each neighbor and its corresponding normal vector. This followed by calculating the orthogonal distances of the source point w.r.t. each parameterized surface. The smallest of these orthogonal distances is then selected as the point-2-surface distance. Prior to calculating the distance between two point clouds, the target point cloud is transformed into the coordinate system of the source point cloud. In this way, since the source cloud is to be enhanced with distance information, the source cloud does not need to be transformed back prior to publishing. The transformation is found, by first extracting the frame IDs from header of each point cloud, which can then be used to look it up in the TF tree. Launch-Parameters: number_of_clouds : Number of clouds to process. The input topics are appended with a index as suffix: cloud_0 , cloud_1 , ..., cloud_N-1 . Type: int Default: 2 processing_rate : Rate in (Hz) at which to process the received point cloud. Type: int Default: 1 distance_measure : Enum: {0 = point_2_point , 1 = point_2_surface } Type: Enum: {0 = point_2_point, 1 = point_2_surface} Default: 0 NOTE: The point_2_surface measure is more robust with a higher number of nearest neighbors. Especially when using point clouds from a rotating LiDAR, as a parameterization of a small number of neighbors might lead to a selection of all neighbors on one ring, making the surface estimation less accurate. num_nearest_neighbors : Number of nearest neighbors to consider in the target cloud for each point in the source cloud. Type: int Default: 5 max_distance : Maximum distance at which to truncate. This is also used for normalization when calculating the intensity. Type: float Default: 5.0 clamp_distance_threshold : Distance at which to clamp the source point cloud. Points exceeding the distance are removed. It is truncated to a minimum of 'max_distance'. Type: float Default: FLT_MAX temp_transform : (Only available if 'number_of_clouds = 2') Temporary transform of camera with respect to LiDAR given in the form of XYZ and RPY. The values are to be given as string list separated by whitespace. If left empty or omitted the transformation will be extracted from the TF-tree by using the frame IDs of the images and the clouds. Type: String Default: \"\" Subscribed Topics: cloud_<idx> : Point cloud for which the distance is to be calculated. Type: sensor_msgs::PointCloud2 Published Topics: ~/cloud_<idx>_enhanced : Source point cloud enhanced with the point-wise normalized distance (with the range of [0,1]) to the target point cloud in the intensity field. Type: sensor_msgs::PointCloud2","title":"PointCloud2PointCloudDistance(Nodelet)"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Error in requesting the calibration metadata - Calibration node not initialized \u00b6 The guidance node as well as the UI get metadata on the configuration (e.g. sensor and topic names) from the calibration node by requesting them via a service call. If the calibration node is not initialized, the service request will return with an error which in turn will be printed out by the requesting components. The failure in initialization of the calibration node typically arises if there is an error in the workspace initialization or if the sensor data is not received. An error in reading the workspaces often comes from a misconfiguration or wrong file permissions. Failure in receiving the sensor data could arise from an error in the connection to the platform or a synchronization error in the data from the sensor. When using approximated synchronization when subscribing to the sensor data, try to increase the queue size. Is there a possibility to visually debug the result of the GICP \u00b6 The are a number of debug visualizations within the source code that are currently deactivated using the preprocessor defines #if 0 . For example, these debug visualizations are located after optimizing the coefficients the target detection in the LiDAR data, as well as, after the alignment of two 3D clouds by means of GICP. To use these visualizations to debug, include them in your compilation by setting the preprocessor define to #if 1 . When activated, a PCL Visualization window will pop up after the computation revealing the results of the corresponding algorithm. In this, the white cloud typically represents the target against which the source cloud is aligned, while the red and green cloud represents the not-aligned and aligned source cloud, respectively.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#error-in-requesting-the-calibration-metadata-calibration-node-not-initialized","text":"The guidance node as well as the UI get metadata on the configuration (e.g. sensor and topic names) from the calibration node by requesting them via a service call. If the calibration node is not initialized, the service request will return with an error which in turn will be printed out by the requesting components. The failure in initialization of the calibration node typically arises if there is an error in the workspace initialization or if the sensor data is not received. An error in reading the workspaces often comes from a misconfiguration or wrong file permissions. Failure in receiving the sensor data could arise from an error in the connection to the platform or a synchronization error in the data from the sensor. When using approximated synchronization when subscribing to the sensor data, try to increase the queue size.","title":"Error in requesting the calibration metadata - Calibration node not initialized"},{"location":"troubleshooting/#is-there-a-possibility-to-visually-debug-the-result-of-the-gicp","text":"The are a number of debug visualizations within the source code that are currently deactivated using the preprocessor defines #if 0 . For example, these debug visualizations are located after optimizing the coefficients the target detection in the LiDAR data, as well as, after the alignment of two 3D clouds by means of GICP. To use these visualizations to debug, include them in your compilation by setting the preprocessor define to #if 1 . When activated, a PCL Visualization window will pop up after the computation revealing the results of the corresponding algorithm. In this, the white cloud typically represents the target against which the source cloud is aligned, while the red and green cloud represents the not-aligned and aligned source cloud, respectively.","title":"Is there a possibility to visually debug the result of the GICP"},{"location":"tutorial/","text":"Tutorial \u00b6 To have a unified starting point 'multisensor_calibration' offers a Calibration Configurator which allows to choose and parameterize the desired calibration. This can be started without any launch parameters, just by calling ros2 run multisensor_calibration multi_sensor_calibration The Calibration Configurator provides a graphical user interface to configure and run one of the following sensor calibrations: Extrinsic Camera-LiDAR Calibration Extrinsic LiDAR-LiDAR Calibration Extrinsic Camera-Reference Calibration Extrinsic LiDAR-Reference Calibration Alternatively, the individual calibrations could also be started as standalone nodes without the calibrator. In this, the user has to manually specify the appropriate launch parameters, which is why the use of the Calibration Configurator is recommended. For more information on that please look into the Nodes section. The calibration configurations, together with the results are stored and organized within workspaces . In this, for each sensor calibration an individual workspace folder is generated and all calibration workspaces belonging to a single robot are, in turn, grouped into a robot workspace. Calibration Configurator \u00b6 The Calibration Configurator provides a graphical user interface to configure and run one of the different sensor calibrations. Figure 1: Graphical user interface of the Calibration Configurator. As further discussed on the workspace page , the configurations and results of the individual sensor calibrations are organized within a workspace structure consisting of robot and calibration workspaces. Thus, in addition to the individual calibration configurations, a few robot settings also need to be set. The Calibration Configurator also allows to install workspace templates. This is especially helpful when running the calibration for the first time. For more information on how to add new templates please refer to corresponding section Robot Options \u00b6 Workspace Folder: Select workspace folder of the robot workspace you wish to open. If a folder is entered that does not yet exist, a new workspace will be generated. All the different robot workspace folders are located in the root directory ( Default: $HOME/multisensor_calibration ), which is listed below the corresponding combo box. To select a different root directory, press the push button next to the combo box of the workspace folder. To install workspace templates into the root directory, press the appropriate push button on the right. This will open another dialog, in which the robot and corresponding sensor calibrations can be selected. Robot Name: Name of the robot. URDF Model: Optional URDF model of the robot. Calibration Options \u00b6 When a robot is selected, the calibration type, which is to be executed can be selected by the appropriate combo box: Calibration Type: Calibration type you wish to execute. Depending on the selected calibration type, different calibration options will be made available. At the start, the combo boxes and text edits will be populated with the available data, i.e. topic names or frame IDs. If a chosen configuration of 'Calibration Type', 'Source Sensor Name', and 'Reference Sensor Name' has already been calibrated and a corresponding settings file is found in the calibration workspace, the rest of the options will be set according to the data found in the settings file. In the following the available options for the different calibration types are presented. Extrinsic Camera-LiDAR Calibration Camera Sensor Name: Name of the source camera sensor. Image Topic: Topic name on which the camera images are published. Camera Info Topic: Topic name on which the camera images are published. Image State: Select the state of the image data. This is need in order to choose the correct data from the camera info messages during calibration. DISTORTED: The image directly coming from the sensor, i.e. containing lense distortion. UNDISTORTED: The image has been made free of lense distortion. STEREO_RECTIFIED: The image comes from a stereo camera has been rectified so that the images of both cameras align horizontally. Is Stereo Camera?: Set to true, if camera is to be calibrated as stereo camera. If activated, 'Right Sensor Name' and 'Right Info Topic' also need to be set. Right Sensor Name: Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Right Info Topic: Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Rect Suffix: Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'Image State' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the 'Image State' is STEREO_RECTIFIED this is removed from the frame ID. LiDAR Sensor Name: Name of the reference LiDAR sensor. Point Cloud Topic: Topic name on which the point clouds are published. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Message Synchronization Policy: Select which type of time synchronization is to be used to synchronize the sensor messages. Synchronization Queue Size: Queue size used for the time synchronization. Extrinsic LiDAR-LiDAR Calibration Source LiDAR Sensor Name: Name of the source LiDAR sensor. Source Point Cloud Topic: Topic name on which the source point clouds are published. Reference LiDAR Sensor Name: Name of the reference LiDAR sensor. Reference Point Cloud Topic: Topic name on which the reference point clouds are published. Calibration Target: Configuration file of the calibration target. Currently not editable. Additionally align ground planes? Set to true, to additionally align the ground planes in the sensor data. If true, this also requires the frame ID of an upright frame in order to find the orientation of the ground plane. Upright Frame ID: - ID of frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Message Synchronization Policy: Select which type of time synchronization is to be used to synchronize the sensor messages. Synchronization Queue Size: Queue size used for the time synchronization. Extrinsic Camera-Reference Calibration Camera Sensor Name: Name of the source camera sensor. Image Topic: Topic name on which the camera images are published. Camera Info Topic: Topic name on which the camera images are published. Image State: Select the state of the image data. This is need in order to choose the correct data from the camera info messages during calibration. DISTORTED: The image directly coming from the sensor, i.e. containing lense distortion. UNDISTORTED: The image has been made free of lense distortion. STEREO_RECTIFIED: The image comes from a stereo camera has been rectified so that the images of both cameras align horizontally. Is Stereo Camera?: Set to true, if camera is to be calibrated as stereo camera. If activated, 'Right Sensor Name' and 'Right Info Topic' also need to be set. Right Sensor Name: Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Right Info Topic: Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Rect Suffix: Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'Image State' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the 'Image State' is STEREO_RECTIFIED this is removed from the frame ID. Reference Name: Name of the reference. Reference Frame ID: Frame ID in which the reference data is provided. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Extrinsic LiDAR-Reference Calibration Source LiDAR Sensor Name: Name of the source LiDAR sensor. Source Point Cloud Topic: Topic name on which the source point clouds are published. Reference Name: Name of the reference. Reference Frame ID: Frame ID in which the reference data is provided. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Extrinsic Camera-LiDAR Calibration \u00b6 The camera-LiDAR calibration is comprised of estimating a relative transformation of a given camera sensor with respect to a given LiDAR sensor. The calibration is performed by solving the Perspective-n-Point (P-n-P) problem with the help of a series of 2D-3D-point-correspondences. Figure 1: Graphical user interface of the extrinsic camera-LiDAR calibration after the calibration target has been detected in the camera image and the LiDAR point cloud. In this, the camera image is annotated with the corners of the ArUco markers and the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 1 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the camera images (B), - a view of the target detections in the LiDAR cloud (C), as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously detects and tracks the ArUco marker inside the camera image and selects potential candidates of the calibration target inside the LiDAR cloud. In this, the markers are framed and for the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in the center of the camera image and parallel to the image plane. Place the target at such a distance to the camera, that its width will fill out approximately half of the image width. Make sure that the markers are detected in the camera image and the target structure is highlighted in the LiDAR cloud. If at least 3 markers are detected and the target structure is highlighted, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 1 . In this, the camera image will be annotated with the corners of the detected ArUco markers (color coded with rainbow color map according to the ID of the marker). the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR cloud was estimated correctly. This is important , since accurate 2D-3D correspondences between the marker corners in the camera image and the LiDAR cloud are needed for the P-n-P algorithm to estimate the extrinsic sensor pose. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (6-7) different poses of the calibration target by placing it at the top and/or bottom edge of the image, at the left and/or right edge of the image, and further away. At each position, slightly rotate the target inwards. Ideally place the target in a '+' shape in the image space with 5 positions. If not possible, try to place the target at least at 3 positions in a 'L' shape so that the positions are not collinear. For each position repeat steps 2-5 . When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, overall mean reprojection error, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. For a good final calibration a typical mean reprojection error is between 2-4 pixels, a translational (XYZ) standard deviation of 1-3 cm, and a rotational (RPY) standard deviation of 0.1-1.5 degrees. After finalization, the calibration can be visualized by clicking the Button 3 'Visualize Current Calibration' . This will open a new dialog with the projection of the LiDAR cloud into the camera image and colorized according to their distance from the camera. Thus, if the calibration is good, then the structure in the LiDAR scan (which can be derived from the depth coloring) should align with the object in the image. the calibration results, as well as a snippet for the URDF file is written into calibration workspace . The workspace folder containing the result files can directly be opened by navigating through the program menu 'File' -> 'Open' -> 'Calibration Workspace' . To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'. Extrinsic LiDAR-LiDAR Calibration \u00b6 The LiDAR-LiDAR calibration is comprised of estimating a relative transformation of a given source LiDAR sensor with respect to a given reference LiDAR sensor. The calibration is performed by aligning a cloud of the source sensor, holding detections of a calibration target, with the cloud of the reference sensor, holding the same observations, by means of GICP. Figure 2: Graphical user interface of the extrinsic LiDAR-LiDAR calibration after the calibration target has been detected in the data of the LiDAR scanners. In this, the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 2 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the source LiDAR cloud (B), - a view of the target detections in the reference LiDAR cloud (C), as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously selects potential candidates of the calibration target inside the LiDAR clouds. In this, the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in front of the sensor for which the extrinsic pose is to be calibrated (i.e the source sensor), while at the same time making sure it can also bee seen from the reference sensor. The target should be placed at such a distance, that the circular cutouts on the calibration target can be well identified. In this, a rule of thumb would be that if possible 2-3 scan lines should pass through or touch a cutout Make sure that the structure of the calibration target is highlighted in the data of both sensors, i.e. in window B and C. If the target structure is highlighted in the data of both LiDAR scanners, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 2 . In this, the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR clouds was estimated correctly. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (3-5) different poses of the calibration target by placing it at a higher or lower position in front of the source sensor. in front of the reference censor, and further away. In this the positions should make up an 'L' shape, so that a vertical, as well as a horizontal variance is established. Make sure that the positions are not collinear. At each position, slightly rotate the target inwards. For each position repeat steps 2-5 . When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, the overall root mean squared error of the GICP, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. For a good final calibration a typical root mean squared error is between 3-8 cm, a translational (XYZ) standard deviation of 0.5-5 cm, and a rotational (RPY) standard deviation of 0.3-2 degrees. After finalization, the calibration can be visualized by clicking the Button 3 'Visualize Calibration' . This will open a new dialog visualizing the point-wise distance between the two point clouds. In this, the point-wise distance visualized with a rainbow colormap (red -> yellow -> green -> blue -> violet). This means, that if the calibration is good, the point-wise distance in overlapping regions should be small and, in turn, the corresponding points should ideally be highlighted in red. the calibration results, as well as a snippet for the URDF file is written into calibration workspace . The workspace folder containing the result files can directly be opened by navigating through the program menu 'File' -> 'Open' -> 'Calibration Workspace' . To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'. Extrinsic Camera-Reference Calibration \u00b6 The camera-reference calibration is comprised of estimating a relative transformation of a given camera sensor with respect to a reference. Similar to the camera-LiDAR calibration, the transformation is calculated by solving the Perspective-n-Point (P-n-P) problem with the help of a series of 2D-3D-point-correspondences. However, instead of finding the 3D correspondences by detecting the calibration target in LiDAR data, they can be entered manually by providing the 3D coordinates of the top-left corners of the ArUco markers with respect to a given reference frame ID. Figure 3: Graphical user interface of the extrinsic camera-reference calibration after the calibration target has been detected in the data of the camera. In this, the camera image is annotated with the corners of the ArUco markers. After starting up, the GUI, as depicted in Figure 3 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the camera images (B), - a table view (C) which allows to enter the XYZ coordinated of the top-left marker corners, as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously detects and tracks the ArUco marker inside the camera image, framing the ArUco markers. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in the center of the camera image and parallel to the image plane. Place the target at such a distance to the camera, that its width will fill out approximately half of the image width. Make sure that the markers are detected in the camera image. If at least 3 markers are detected, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 3 . In this, the camera image will be annotated with the corners of the detected ArUco markers (color coded with rainbow color map according to the ID of the marker). If the detection was faulty, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (6-7) different poses of the calibration target by placing it at the top and/or bottom edge of the image, at the left and/or right edge of the image, and further away. At each position, slightly rotate the target inwards. Ideally place the target in a '+' shape in the image space with 5 positions. If not possible, try to place the target at least at 3 positions in a 'L' shape so that the positions are not collinear. For each position repeat steps 2-4 . After all target poses were captured in the camera image, fill in reference coordinates of the top-left marker corners in the appropriate table, together with the corresponding ID of the target pose and the corresponding marker ID. To this end, it is assumed that the observed target poses are entered in a contiguous order, starting off with the ID 1. Meaning, that the first position of the target gets the ID 1, the second the ID 2, and so on. Furthermore, for each target pose there are to be 4 markers given. You can also enter the reference coordinated successively while you move the target around, however, it is important that the observations are only 'applied' once at the end , when all coordinates have been entered. When enough different observations of the calibration target are collected and the reference coordinates of the top-left marker corners have been entered and 'applied', the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, overall mean reprojection error, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'. Extrinsic LiDAR-Reference Calibration \u00b6 The calibration is performed by aligning a cloud of the source sensor, holding detections of a calibration target, with the cloud of the reference sensor, holding the same observations, by means of GICP. The LiDAR-reference calibration is comprised of estimating a relative transformation of a given source LiDAR sensor with respect to a reference. Similar to the LiDAR-LiDAR calibration, the transformation is calculated by aligning a cloud of the source sensor, holding detections of a calibration target, with the reference data by means of GICP. However, instead of finding the 3D correspondences by detecting the calibration target in the reference LiDAR data, they can be entered manually by providing the 3D coordinates of the top-left corners of the ArUco markers with respect to a given reference frame ID. Figure 4: Graphical user interface of the extrinsic LiDAR-reference calibration after the calibration target has been detected in the data of the camera. In this, the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 4 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the source LiDAR cloud (B), - a table view (C) which allows to enter the XYZ coordinated of the top-left marker corners, as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously selects potential candidates of the calibration target inside the LiDAR clouds. In this, the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in front of the sensor for which the extrinsic pose is to be calibrated (i.e the source sensor). The target should be placed at such a distance, that the circular cutouts on the calibration target can be well identified. In this, a rule of thumb would be that if possible 2-3 scan lines should pass through or touch a cutout Make sure that the structure of the calibration target is highlighted in the data of the sensor, i.e. in window B. If the target structure is highlighted in the data of the LiDAR scanners, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 4 . In this, the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR cloud was estimated correctly. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (3-5) different poses of the calibration target by placing it at a higher or lower position in front of the source sensor, and further away. In this the positions should make up an 'L' shape, so that a vertical, as well as a horizontal variance is established. Make sure that the positions are not collinear. At each position, slightly rotate the target inwards. For each position repeat steps 2-5 . After all target poses were captured in the camera image, fill in reference coordinates of the top-left marker corners in the appropriate table, together with the corresponding ID of the target pose and the corresponding marker ID. To this end, it is assumed that the observed target poses are entered in a contiguous order, starting off with the ID 1. Meaning, that the first position of the target gets the ID 1, the second the ID 2, and so on. Furthermore, for each target pose there are to be 4 markers given. You can also enter the reference coordinated successively while you move the target around, however, it is important that the observations are only 'applied' once at the end , when all coordinates have been entered. When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, the overall root mean squared error of the GICP, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'.","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"To have a unified starting point 'multisensor_calibration' offers a Calibration Configurator which allows to choose and parameterize the desired calibration. This can be started without any launch parameters, just by calling ros2 run multisensor_calibration multi_sensor_calibration The Calibration Configurator provides a graphical user interface to configure and run one of the following sensor calibrations: Extrinsic Camera-LiDAR Calibration Extrinsic LiDAR-LiDAR Calibration Extrinsic Camera-Reference Calibration Extrinsic LiDAR-Reference Calibration Alternatively, the individual calibrations could also be started as standalone nodes without the calibrator. In this, the user has to manually specify the appropriate launch parameters, which is why the use of the Calibration Configurator is recommended. For more information on that please look into the Nodes section. The calibration configurations, together with the results are stored and organized within workspaces . In this, for each sensor calibration an individual workspace folder is generated and all calibration workspaces belonging to a single robot are, in turn, grouped into a robot workspace.","title":"Tutorial"},{"location":"tutorial/#calibration-configurator","text":"The Calibration Configurator provides a graphical user interface to configure and run one of the different sensor calibrations. Figure 1: Graphical user interface of the Calibration Configurator. As further discussed on the workspace page , the configurations and results of the individual sensor calibrations are organized within a workspace structure consisting of robot and calibration workspaces. Thus, in addition to the individual calibration configurations, a few robot settings also need to be set. The Calibration Configurator also allows to install workspace templates. This is especially helpful when running the calibration for the first time. For more information on how to add new templates please refer to corresponding section","title":"Calibration Configurator"},{"location":"tutorial/#robot-options","text":"Workspace Folder: Select workspace folder of the robot workspace you wish to open. If a folder is entered that does not yet exist, a new workspace will be generated. All the different robot workspace folders are located in the root directory ( Default: $HOME/multisensor_calibration ), which is listed below the corresponding combo box. To select a different root directory, press the push button next to the combo box of the workspace folder. To install workspace templates into the root directory, press the appropriate push button on the right. This will open another dialog, in which the robot and corresponding sensor calibrations can be selected. Robot Name: Name of the robot. URDF Model: Optional URDF model of the robot.","title":"Robot Options"},{"location":"tutorial/#calibration-options","text":"When a robot is selected, the calibration type, which is to be executed can be selected by the appropriate combo box: Calibration Type: Calibration type you wish to execute. Depending on the selected calibration type, different calibration options will be made available. At the start, the combo boxes and text edits will be populated with the available data, i.e. topic names or frame IDs. If a chosen configuration of 'Calibration Type', 'Source Sensor Name', and 'Reference Sensor Name' has already been calibrated and a corresponding settings file is found in the calibration workspace, the rest of the options will be set according to the data found in the settings file. In the following the available options for the different calibration types are presented. Extrinsic Camera-LiDAR Calibration Camera Sensor Name: Name of the source camera sensor. Image Topic: Topic name on which the camera images are published. Camera Info Topic: Topic name on which the camera images are published. Image State: Select the state of the image data. This is need in order to choose the correct data from the camera info messages during calibration. DISTORTED: The image directly coming from the sensor, i.e. containing lense distortion. UNDISTORTED: The image has been made free of lense distortion. STEREO_RECTIFIED: The image comes from a stereo camera has been rectified so that the images of both cameras align horizontally. Is Stereo Camera?: Set to true, if camera is to be calibrated as stereo camera. If activated, 'Right Sensor Name' and 'Right Info Topic' also need to be set. Right Sensor Name: Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Right Info Topic: Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Rect Suffix: Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'Image State' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the 'Image State' is STEREO_RECTIFIED this is removed from the frame ID. LiDAR Sensor Name: Name of the reference LiDAR sensor. Point Cloud Topic: Topic name on which the point clouds are published. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Message Synchronization Policy: Select which type of time synchronization is to be used to synchronize the sensor messages. Synchronization Queue Size: Queue size used for the time synchronization. Extrinsic LiDAR-LiDAR Calibration Source LiDAR Sensor Name: Name of the source LiDAR sensor. Source Point Cloud Topic: Topic name on which the source point clouds are published. Reference LiDAR Sensor Name: Name of the reference LiDAR sensor. Reference Point Cloud Topic: Topic name on which the reference point clouds are published. Calibration Target: Configuration file of the calibration target. Currently not editable. Additionally align ground planes? Set to true, to additionally align the ground planes in the sensor data. If true, this also requires the frame ID of an upright frame in order to find the orientation of the ground plane. Upright Frame ID: - ID of frame which has an upwards pointing z-axis. Used to detect ground plane in sensor data. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Message Synchronization Policy: Select which type of time synchronization is to be used to synchronize the sensor messages. Synchronization Queue Size: Queue size used for the time synchronization. Extrinsic Camera-Reference Calibration Camera Sensor Name: Name of the source camera sensor. Image Topic: Topic name on which the camera images are published. Camera Info Topic: Topic name on which the camera images are published. Image State: Select the state of the image data. This is need in order to choose the correct data from the camera info messages during calibration. DISTORTED: The image directly coming from the sensor, i.e. containing lense distortion. UNDISTORTED: The image has been made free of lense distortion. STEREO_RECTIFIED: The image comes from a stereo camera has been rectified so that the images of both cameras align horizontally. Is Stereo Camera?: Set to true, if camera is to be calibrated as stereo camera. If activated, 'Right Sensor Name' and 'Right Info Topic' also need to be set. Right Sensor Name: Name of the right camera sensor when the camera is to be calibrated as a stereo camera system. Right Info Topic: Topic name of the camera info corresponding to the right camera. This is needed when the camera is to be calibrated as a stereo camera system. Rect Suffix: Suffix of the of the right sensor name as well as the frame id for the rectified images. If the 'Image State' of the input images is DISTORTED or UNDISTORTED, this is added to the rectified frame id. If the 'Image State' is STEREO_RECTIFIED this is removed from the frame ID. Reference Name: Name of the reference. Reference Frame ID: Frame ID in which the reference data is provided. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace. Extrinsic LiDAR-Reference Calibration Source LiDAR Sensor Name: Name of the source LiDAR sensor. Source Point Cloud Topic: Topic name on which the source point clouds are published. Reference Name: Name of the reference. Reference Frame ID: Frame ID in which the reference data is provided. Calibration Target: Configuration file of the calibration target. Currently not editable. Calibrate with respect to base frame? Calibrate the pose of the source sensor with respect to a base frame. Base Frame ID: Frame ID of the base frame with respect to which the pose is to be calibrated. Use initial guess from TF-Tree? Use the data provided by the TF-Tree to extract an initial guess for the calibration. Save Observations? Save the observations which have been used for the calibration into the calibration workspace.","title":"Calibration Options"},{"location":"tutorial/#extrinsic-camera-lidar-calibration","text":"The camera-LiDAR calibration is comprised of estimating a relative transformation of a given camera sensor with respect to a given LiDAR sensor. The calibration is performed by solving the Perspective-n-Point (P-n-P) problem with the help of a series of 2D-3D-point-correspondences. Figure 1: Graphical user interface of the extrinsic camera-LiDAR calibration after the calibration target has been detected in the camera image and the LiDAR point cloud. In this, the camera image is annotated with the corners of the ArUco markers and the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 1 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the camera images (B), - a view of the target detections in the LiDAR cloud (C), as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously detects and tracks the ArUco marker inside the camera image and selects potential candidates of the calibration target inside the LiDAR cloud. In this, the markers are framed and for the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in the center of the camera image and parallel to the image plane. Place the target at such a distance to the camera, that its width will fill out approximately half of the image width. Make sure that the markers are detected in the camera image and the target structure is highlighted in the LiDAR cloud. If at least 3 markers are detected and the target structure is highlighted, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 1 . In this, the camera image will be annotated with the corners of the detected ArUco markers (color coded with rainbow color map according to the ID of the marker). the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR cloud was estimated correctly. This is important , since accurate 2D-3D correspondences between the marker corners in the camera image and the LiDAR cloud are needed for the P-n-P algorithm to estimate the extrinsic sensor pose. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (6-7) different poses of the calibration target by placing it at the top and/or bottom edge of the image, at the left and/or right edge of the image, and further away. At each position, slightly rotate the target inwards. Ideally place the target in a '+' shape in the image space with 5 positions. If not possible, try to place the target at least at 3 positions in a 'L' shape so that the positions are not collinear. For each position repeat steps 2-5 . When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, overall mean reprojection error, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. For a good final calibration a typical mean reprojection error is between 2-4 pixels, a translational (XYZ) standard deviation of 1-3 cm, and a rotational (RPY) standard deviation of 0.1-1.5 degrees. After finalization, the calibration can be visualized by clicking the Button 3 'Visualize Current Calibration' . This will open a new dialog with the projection of the LiDAR cloud into the camera image and colorized according to their distance from the camera. Thus, if the calibration is good, then the structure in the LiDAR scan (which can be derived from the depth coloring) should align with the object in the image. the calibration results, as well as a snippet for the URDF file is written into calibration workspace . The workspace folder containing the result files can directly be opened by navigating through the program menu 'File' -> 'Open' -> 'Calibration Workspace' . To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'.","title":"Extrinsic Camera-LiDAR Calibration"},{"location":"tutorial/#extrinsic-lidar-lidar-calibration","text":"The LiDAR-LiDAR calibration is comprised of estimating a relative transformation of a given source LiDAR sensor with respect to a given reference LiDAR sensor. The calibration is performed by aligning a cloud of the source sensor, holding detections of a calibration target, with the cloud of the reference sensor, holding the same observations, by means of GICP. Figure 2: Graphical user interface of the extrinsic LiDAR-LiDAR calibration after the calibration target has been detected in the data of the LiDAR scanners. In this, the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 2 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the source LiDAR cloud (B), - a view of the target detections in the reference LiDAR cloud (C), as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously selects potential candidates of the calibration target inside the LiDAR clouds. In this, the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in front of the sensor for which the extrinsic pose is to be calibrated (i.e the source sensor), while at the same time making sure it can also bee seen from the reference sensor. The target should be placed at such a distance, that the circular cutouts on the calibration target can be well identified. In this, a rule of thumb would be that if possible 2-3 scan lines should pass through or touch a cutout Make sure that the structure of the calibration target is highlighted in the data of both sensors, i.e. in window B and C. If the target structure is highlighted in the data of both LiDAR scanners, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 2 . In this, the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR clouds was estimated correctly. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (3-5) different poses of the calibration target by placing it at a higher or lower position in front of the source sensor. in front of the reference censor, and further away. In this the positions should make up an 'L' shape, so that a vertical, as well as a horizontal variance is established. Make sure that the positions are not collinear. At each position, slightly rotate the target inwards. For each position repeat steps 2-5 . When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, the overall root mean squared error of the GICP, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. For a good final calibration a typical root mean squared error is between 3-8 cm, a translational (XYZ) standard deviation of 0.5-5 cm, and a rotational (RPY) standard deviation of 0.3-2 degrees. After finalization, the calibration can be visualized by clicking the Button 3 'Visualize Calibration' . This will open a new dialog visualizing the point-wise distance between the two point clouds. In this, the point-wise distance visualized with a rainbow colormap (red -> yellow -> green -> blue -> violet). This means, that if the calibration is good, the point-wise distance in overlapping regions should be small and, in turn, the corresponding points should ideally be highlighted in red. the calibration results, as well as a snippet for the URDF file is written into calibration workspace . The workspace folder containing the result files can directly be opened by navigating through the program menu 'File' -> 'Open' -> 'Calibration Workspace' . To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'.","title":"Extrinsic LiDAR-LiDAR Calibration"},{"location":"tutorial/#extrinsic-camera-reference-calibration","text":"The camera-reference calibration is comprised of estimating a relative transformation of a given camera sensor with respect to a reference. Similar to the camera-LiDAR calibration, the transformation is calculated by solving the Perspective-n-Point (P-n-P) problem with the help of a series of 2D-3D-point-correspondences. However, instead of finding the 3D correspondences by detecting the calibration target in LiDAR data, they can be entered manually by providing the 3D coordinates of the top-left corners of the ArUco markers with respect to a given reference frame ID. Figure 3: Graphical user interface of the extrinsic camera-reference calibration after the calibration target has been detected in the data of the camera. In this, the camera image is annotated with the corners of the ArUco markers. After starting up, the GUI, as depicted in Figure 3 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the camera images (B), - a table view (C) which allows to enter the XYZ coordinated of the top-left marker corners, as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously detects and tracks the ArUco marker inside the camera image, framing the ArUco markers. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in the center of the camera image and parallel to the image plane. Place the target at such a distance to the camera, that its width will fill out approximately half of the image width. Make sure that the markers are detected in the camera image. If at least 3 markers are detected, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 3 . In this, the camera image will be annotated with the corners of the detected ArUco markers (color coded with rainbow color map according to the ID of the marker). If the detection was faulty, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (6-7) different poses of the calibration target by placing it at the top and/or bottom edge of the image, at the left and/or right edge of the image, and further away. At each position, slightly rotate the target inwards. Ideally place the target in a '+' shape in the image space with 5 positions. If not possible, try to place the target at least at 3 positions in a 'L' shape so that the positions are not collinear. For each position repeat steps 2-4 . After all target poses were captured in the camera image, fill in reference coordinates of the top-left marker corners in the appropriate table, together with the corresponding ID of the target pose and the corresponding marker ID. To this end, it is assumed that the observed target poses are entered in a contiguous order, starting off with the ID 1. Meaning, that the first position of the target gets the ID 1, the second the ID 2, and so on. Furthermore, for each target pose there are to be 4 markers given. You can also enter the reference coordinated successively while you move the target around, however, it is important that the observations are only 'applied' once at the end , when all coordinates have been entered. When enough different observations of the calibration target are collected and the reference coordinates of the top-left marker corners have been entered and 'applied', the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, overall mean reprojection error, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'.","title":"Extrinsic Camera-Reference Calibration"},{"location":"tutorial/#extrinsic-lidar-reference-calibration","text":"The calibration is performed by aligning a cloud of the source sensor, holding detections of a calibration target, with the cloud of the reference sensor, holding the same observations, by means of GICP. The LiDAR-reference calibration is comprised of estimating a relative transformation of a given source LiDAR sensor with respect to a reference. Similar to the LiDAR-LiDAR calibration, the transformation is calculated by aligning a cloud of the source sensor, holding detections of a calibration target, with the reference data by means of GICP. However, instead of finding the 3D correspondences by detecting the calibration target in the reference LiDAR data, they can be entered manually by providing the 3D coordinates of the top-left corners of the ArUco markers with respect to a given reference frame ID. Figure 4: Graphical user interface of the extrinsic LiDAR-reference calibration after the calibration target has been detected in the data of the camera. In this, the detected target structure is highlighted with yellow in the LiDAR point cloud. After starting up, the GUI, as depicted in Figure 4 , will appear. It is made up of, - a 'Control Window' (A), - a view on the target detections in the source LiDAR cloud (B), - a table view (C) which allows to enter the XYZ coordinated of the top-left marker corners, as well as - a view on a dialog that is supposed to guide the user in the placement of the calibration target (D) (currently in development) . The calibration node continuously selects potential candidates of the calibration target inside the LiDAR clouds. In this, the points of the target candidates in the point cloud are displayed with the corresponding intensity value. To perform an extrinsic calibration, follow the steps below: Start off by placing the calibration target in front of the sensor for which the extrinsic pose is to be calibrated (i.e the source sensor). The target should be placed at such a distance, that the circular cutouts on the calibration target can be well identified. In this, a rule of thumb would be that if possible 2-3 scan lines should pass through or touch a cutout Make sure that the structure of the calibration target is highlighted in the data of the sensor, i.e. in window B. If the target structure is highlighted in the data of the LiDAR scanners, click on Button 1 'Capture Target Observation' to trigger full detection and precise estimation of the calibration target in the sensor data and to add the detection to the observation list. When finished the corresponding sensor data will be annotated with the detections as seen in Figure 4 . In this, the points of the LiDAR cloud that correspond to the detected calibration target are highlighted in yellow. In addition, from the estimated pose of the target, the corners of the ArUco markers are also drawn in the same color scheme as in the camera image. Perform a visual inspection on whether the orientation of the calibration target in the LiDAR cloud was estimated correctly. If the orientation was not correctly estimated, or if a wrong structure in the LiDAR cloud was supposedly marked as the calibration target, or any other fault, you can remove the last observation from the stack by clicking on Button 2 'Remove Last Observation' . This can be repeated multiple times as it will always remove the top-most observation on the stack until no observation is left. Continue to add (3-5) different poses of the calibration target by placing it at a higher or lower position in front of the source sensor, and further away. In this the positions should make up an 'L' shape, so that a vertical, as well as a horizontal variance is established. Make sure that the positions are not collinear. At each position, slightly rotate the target inwards. For each position repeat steps 2-5 . After all target poses were captured in the camera image, fill in reference coordinates of the top-left marker corners in the appropriate table, together with the corresponding ID of the target pose and the corresponding marker ID. To this end, it is assumed that the observed target poses are entered in a contiguous order, starting off with the ID 1. Meaning, that the first position of the target gets the ID 1, the second the ID 2, and so on. Furthermore, for each target pose there are to be 4 markers given. You can also enter the reference coordinated successively while you move the target around, however, it is important that the observations are only 'applied' once at the end , when all coordinates have been entered. When enough different observations of the calibration target are collected, the calibration can be finalized by clicking on Button 4 'Calibrate' . This will do a final calibration, print out the resulting extrinsic parameters, the overall root mean squared error of the GICP, as well as a standard deviation in the target poses when transforming it from the source sensor to the reference sensor. To adjust runtime parameters, e.g. parameters of the algorithm to detect the calibration target or to do the actual calibration algorithm, you can open 'rqt_reconfigure' via 'Edit' -> 'Preferences'.","title":"Extrinsic LiDAR-Reference Calibration"},{"location":"workspaces/","text":"Workspaces \u00b6 The calibration uses a workspace to store the settings and the calibration results. In this, the idea is to use one workspace per robot ( Robot Workspace ) in which different sub-workspaces ( Calibration Workspace ) are created for each sensor and calibration type. A typical workspace structure looks as follows: <RobotName> | settings.ini | <SomeUrdfFile>.urdf | <SomeUrdfFile>_<LastDateModified>.urdf | |----<src_sensor_name>_<ref_sensor_name>_<intrinsic|extrinsic>_calibration | | settings.ini | | calibration_results.txt | | urdf_snippet.txt | | <calibration_target>.yaml | | | |---observations | |---_backups | | | ... | |----<src_sensor_name>_<ref_sensor_name>_<intrinsic|extrinsic>_calibration | | ... | | ... The default root folder of all workspaces is $HOME/multisensor_calibration Details on the content of the different workspaces are discussed in the following. Robot Workspace \u00b6 The root directory (i.e. the robot workspace) as well as each subdirectory (i.e. the calibration workspaces) hold a settings file ( settings.ini ) which holds the robot and calibration specific settings, respectively. The robot settings hold the following parameters: name : name of the robot urdf_model_path : Optional path to URDF model of robot. When a relative path is given, it is assumed that this is relative to the robot workspace folder. If an URDF Model is provided the extrinsic calibration results are directly written into the URDF file. Furthermore, the extrinsic pose computed by the calibration is directly stored into the URDF file if it is available. In this case the existing file is renamed with the date of the last modification appended to the filename. The calibration can still be performed, even if no urdf file is provided. A warning message will be emitted nonetheless. Initialize New Robot Workspace \u00b6 To initialize a new robot workspace there are two options: Run the Calibration Configurator and start a calibration with the parameterization of the new workspace. The configurator will create and initialize a new workspace if it doesn't exist. Alternatively, one can use the initialize_robot_workspace node. Simply run: ros2 run multisensor_calibration initialize_robot_workspace --ros-args -p robot_ws_path:=<path_to_ws_dir> -p robot_name:=<robot_name> [-p urdf_model_path:=<path_to_file>] Calibration Workspace \u00b6 The calibration settings comprise the calibration specific settings and correspond to the launch parameters of the different calibration routines as stated below. After each calibration the results are stored in a calibration_results.txt file in the corresponding subdirectory. In addition to the calibration results, an XML block is written into urdf_snippet.txt with the calibration data which can be copied into a URDF file. If specified in the launch configuration of the calibration nodes, the captured observations will be stored in observations . At the start of a new calibration previous calibrations will be backed-up and moved into a new subfolder within _backups . If a URDF file is specified in the robot settings and if an entry for the calibrated sensor exists in this file, the extrinsic calibration results are directly written into the URDF file. Workspace Templates \u00b6 The Calibration Configurator also allows to install workspace templates. This is especially helpful when running the calibration for the first time. By default the calibration configurator has a workspace template for an example robot. If the Multi-Sensor Calibration toolbox is build and installed from source, however, one can provide the configurator with more customized workspace templates. To do so: Create a folder structure somewhere in the file system with (multiple) robot workspaces, each containing the desired calibration workspaces as depicted above. Within each robot workspace folder create a 'settings.ini' file according to this template . And within each calibration workspace create a 'settings.ini' file according to the appropriate template: extrinsic_camera_lidar_workspace_settings_template extrinsic_lidar_lidar_workspace_settings_template extrinsic_camera_reference_workspace_settings_template extrinsic_lidar_reference_workspace_settings_template NOTE: In this it is important that the stated workspace type corresponds to the calibration type Call the script within the repository called 'scripts/populate_robot_workspaces_qrc.sh' with the path to the root directory of the custom workspace structure as argument prior to building 'multisensor_calibration'. This will copy the custom workspaces an populate the corresponding resource file so that the calibration configurator access them for installation.","title":"Workspace Handling"},{"location":"workspaces/#workspaces","text":"The calibration uses a workspace to store the settings and the calibration results. In this, the idea is to use one workspace per robot ( Robot Workspace ) in which different sub-workspaces ( Calibration Workspace ) are created for each sensor and calibration type. A typical workspace structure looks as follows: <RobotName> | settings.ini | <SomeUrdfFile>.urdf | <SomeUrdfFile>_<LastDateModified>.urdf | |----<src_sensor_name>_<ref_sensor_name>_<intrinsic|extrinsic>_calibration | | settings.ini | | calibration_results.txt | | urdf_snippet.txt | | <calibration_target>.yaml | | | |---observations | |---_backups | | | ... | |----<src_sensor_name>_<ref_sensor_name>_<intrinsic|extrinsic>_calibration | | ... | | ... The default root folder of all workspaces is $HOME/multisensor_calibration Details on the content of the different workspaces are discussed in the following.","title":"Workspaces"},{"location":"workspaces/#robot-workspace","text":"The root directory (i.e. the robot workspace) as well as each subdirectory (i.e. the calibration workspaces) hold a settings file ( settings.ini ) which holds the robot and calibration specific settings, respectively. The robot settings hold the following parameters: name : name of the robot urdf_model_path : Optional path to URDF model of robot. When a relative path is given, it is assumed that this is relative to the robot workspace folder. If an URDF Model is provided the extrinsic calibration results are directly written into the URDF file. Furthermore, the extrinsic pose computed by the calibration is directly stored into the URDF file if it is available. In this case the existing file is renamed with the date of the last modification appended to the filename. The calibration can still be performed, even if no urdf file is provided. A warning message will be emitted nonetheless.","title":"Robot Workspace"},{"location":"workspaces/#initialize-new-robot-workspace","text":"To initialize a new robot workspace there are two options: Run the Calibration Configurator and start a calibration with the parameterization of the new workspace. The configurator will create and initialize a new workspace if it doesn't exist. Alternatively, one can use the initialize_robot_workspace node. Simply run: ros2 run multisensor_calibration initialize_robot_workspace --ros-args -p robot_ws_path:=<path_to_ws_dir> -p robot_name:=<robot_name> [-p urdf_model_path:=<path_to_file>]","title":"Initialize New Robot Workspace"},{"location":"workspaces/#calibration-workspace","text":"The calibration settings comprise the calibration specific settings and correspond to the launch parameters of the different calibration routines as stated below. After each calibration the results are stored in a calibration_results.txt file in the corresponding subdirectory. In addition to the calibration results, an XML block is written into urdf_snippet.txt with the calibration data which can be copied into a URDF file. If specified in the launch configuration of the calibration nodes, the captured observations will be stored in observations . At the start of a new calibration previous calibrations will be backed-up and moved into a new subfolder within _backups . If a URDF file is specified in the robot settings and if an entry for the calibrated sensor exists in this file, the extrinsic calibration results are directly written into the URDF file.","title":"Calibration Workspace"},{"location":"workspaces/#workspace-templates","text":"The Calibration Configurator also allows to install workspace templates. This is especially helpful when running the calibration for the first time. By default the calibration configurator has a workspace template for an example robot. If the Multi-Sensor Calibration toolbox is build and installed from source, however, one can provide the configurator with more customized workspace templates. To do so: Create a folder structure somewhere in the file system with (multiple) robot workspaces, each containing the desired calibration workspaces as depicted above. Within each robot workspace folder create a 'settings.ini' file according to this template . And within each calibration workspace create a 'settings.ini' file according to the appropriate template: extrinsic_camera_lidar_workspace_settings_template extrinsic_lidar_lidar_workspace_settings_template extrinsic_camera_reference_workspace_settings_template extrinsic_lidar_reference_workspace_settings_template NOTE: In this it is important that the stated workspace type corresponds to the calibration type Call the script within the repository called 'scripts/populate_robot_workspaces_qrc.sh' with the path to the root directory of the custom workspace structure as argument prior to building 'multisensor_calibration'. This will copy the custom workspaces an populate the corresponding resource file so that the calibration configurator access them for installation.","title":"Workspace Templates"}]}